{
 "cells": [
  {
   "cell_type": "raw",
   "id": "078af787-996a-46bb-95a7-ba61a9a3bfd8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Quiz 4: Vectorized Gradient Descent\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    number-sections: true\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3136e9-367d-4da4-ba65-ca837676f0cf",
   "metadata": {},
   "source": [
    "![vectorization](https://miro.medium.com/v2/resize:fit:640/format:webp/1*V_SlJTij8lH8-bxctCEivg.png)\n",
    "\n",
    "## Matrix Multiplication and Vectorization in Machine Learning and Deep Learning\n",
    "\n",
    "Matrix multiplication and vectorization are cornerstone techniques in modern machine learning, transforming how we implement and optimize algorithms. Understanding these concepts is crucial for building efficient, scalable machine learning systems.\n",
    "\n",
    "### Why Vectorization Matters in Machine Learning\n",
    "\n",
    "**Computational Efficiency**\n",
    "\n",
    "- **Hardware Optimization**: Libraries like NumPy, TensorFlow, and PyTorch are built on optimized low-level implementations (C/C++/CUDA)\n",
    "- **SIMD Operations**: Single Instruction, Multiple Data operations allow processing multiple elements simultaneously\n",
    "- **Performance Gains**: Vectorized operations can be 10-100x faster than equivalent Python loops\n",
    "- **Memory Efficiency**: Reduced memory allocation overhead and better cache utilization\n",
    "\n",
    "**Algorithmic Simplicity**\n",
    "\n",
    "- **Concise Code**: Complex mathematical operations become single-line expressions\n",
    "- **Reduced Bugs**: Fewer loops mean fewer opportunities for indexing errors\n",
    "- **Mathematical Clarity**: Code structure mirrors mathematical notation more closely\n",
    "- **Maintainability**: Easier to read, understand, and modify vectorized implementations\n",
    "\n",
    "**Parallel Processing Power**\n",
    "\n",
    "- **Multi-core CPUs**: Automatic distribution across available CPU cores\n",
    "- **GPU Acceleration**: Seamless utilization of thousands of GPU cores for massive parallelism\n",
    "- **Distributed Computing**: Efficient scaling across multiple machines\n",
    "- **Asynchronous Operations**: Non-blocking computations for better resource utilization\n",
    "\n",
    "**Batch Processing Excellence**\n",
    "\n",
    "- **Training Efficiency**: Process entire datasets or mini-batches simultaneously\n",
    "- **Neural Networks**: Essential for forward and backward propagation across multiple samples\n",
    "- **Feature Engineering**: Apply transformations to all samples at once\n",
    "- **Prediction Speed**: Generate predictions for thousands of samples in milliseconds\n",
    "\n",
    "**Optimization Acceleration**\n",
    "\n",
    "- **Gradient Descent**: Vectorized gradient computations for faster convergence\n",
    "- **Matrix Factorization**: Efficient decomposition techniques for dimensionality reduction\n",
    "- **Regularization**: Apply penalties across all parameters simultaneously\n",
    "- **Cross-validation**: Parallel model evaluation across different folds\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "Consider training a neural network on 10,000 samples with 100 features:\n",
    "- **Loop-based approach**: ~10 seconds per epoch\n",
    "- **Vectorized approach**: ~0.1 seconds per epoch\n",
    "\n",
    "This 100x speedup means the difference between waiting hours vs. minutes for model training, enabling rapid experimentation and iteration in machine learning workflows.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Vectorization leverages the mathematical properties of linear algebra to transform element-wise operations into matrix operations, which are inherently more efficient and parallelizable. This transformation is the foundation that makes modern deep learning feasible on large-scale datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbb106-aa5b-488f-aada-0781761a19ed",
   "metadata": {},
   "source": [
    "### Prerequisite: Matrix Operations in NumPy\n",
    "\n",
    "#### Matrix Fundamentals\n",
    "Matrices are two-dimensional arrays where all elements are of the same data type. They form the backbone of linear algebra operations in machine learning and are essential for efficient computation.\n",
    "\n",
    "#### Mathematical Notation and Conventions\n",
    "- **Matrix Notation**: Matrices are denoted with capital, bold letters such as $\\mathbf{X}$, $\\mathbf{W}$, $\\mathbf{A}$\n",
    "- **Dimensions**: \n",
    "  - $m$ = number of rows (typically represents number of training examples)\n",
    "  - $n$ = number of columns (typically represents number of features)\n",
    "  - Matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ has $m$ rows and $n$ columns\n",
    "\n",
    "#### Design Matrix with Bias Term\n",
    "In machine learning, we often augment our feature matrix to include a bias term. The bias $b$ is incorporated as the **first column** of the matrix, consisting of all ones. This allows us to combine the bias and weights into a single matrix operation:\n",
    "\n",
    "$$\\mathbf{X}_{\\text{augmented}} = \\begin{bmatrix}\n",
    "1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n",
    "1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times (n+1)}$$\n",
    "\n",
    "**Key Benefits of Bias Integration:**\n",
    "- **Simplified Computation**: $\\mathbf{y} = \\mathbf{X}_{\\text{augmented}} \\mathbf{w}$ instead of $\\mathbf{y} = \\mathbf{X}\\mathbf{w} + b$\n",
    "- **Vectorized Operations**: All parameters (weights + bias) updated simultaneously\n",
    "- **Cleaner Implementation**: Reduces code complexity and potential for errors\n",
    "\n",
    "#### Matrix Element Indexing\n",
    "- **Superscript notation**: $x^{(i)}$ refers to the $i^{th}$ training example\n",
    "- **Subscript notation**: $x_j^{(i)}$ refers to the $j^{th}$ feature of the $i^{th}$ training example\n",
    "- **Python indexing**: Remember that NumPy uses 0-based indexing (first element is index 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11b067-52f0-4afe-a6c9-4e563ba52d55",
   "metadata": {},
   "source": [
    "### Matrix Multiplication in NumPy\n",
    "\n",
    "Matrix multiplication is a fundamental operation in linear algebra and machine learning. NumPy provides multiple methods to perform this operation, each with specific use cases and advantages.\n",
    "\n",
    "#### Three Ways to Multiply Matrices in NumPy\n",
    "\n",
    "**1. `np.dot()` - The Classic Approach**\n",
    "- Most versatile NumPy function for matrix operations\n",
    "- Works with 1D arrays (dot product), 2D arrays (matrix multiplication), and higher dimensions\n",
    "- Legacy method, widely used in older codebases\n",
    "\n",
    "**2. `@` Operator - The Modern Pythonic Way**\n",
    "- Introduced in Python 3.5 (PEP 465)\n",
    "- Cleaner, more readable syntax that mirrors mathematical notation\n",
    "- Specifically designed for matrix multiplication\n",
    "- **Recommended for new code**\n",
    "\n",
    "**3. `np.matmul()` - The Explicit Matrix Multiplication**\n",
    "- Explicitly designed for matrix multiplication\n",
    "- Similar to `@` operator (in fact, `@` calls `matmul` internally)\n",
    "- Useful when you want to emphasize that you're doing matrix multiplication\n",
    "- Handles broadcasting differently than `np.dot()` for higher dimensions\n",
    "\n",
    "#### Matrix Multiplication Rules\n",
    "\n",
    "For matrix multiplication $\\mathbf{C} = \\mathbf{A} \\mathbf{B}$:\n",
    "- $\\mathbf{A}$ has shape $(m, k)$\n",
    "- $\\mathbf{B}$ has shape $(k, n)$\n",
    "- Result $\\mathbf{C}$ has shape $(m, n)$\n",
    "- **Critical Rule**: Number of columns in $\\mathbf{A}$ must equal number of rows in $\\mathbf{B}$\n",
    "\n",
    "#### Example Below\n",
    "In the code cells below, we'll demonstrate all three methods with:\n",
    "- `matrix_a`: shape $(2, 3)$ - 2 rows, 3 columns\n",
    "- `matrix_b`: shape $(3, 2)$ - 3 rows, 2 columns  \n",
    "- `result`: shape $(2, 2)$ - 2 rows, 2 columns\n",
    "\n",
    "All three methods produce identical results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ea15c3-f4fe-4880-9a9a-ab10db0280cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A (2√ó3):\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Matrix B (3√ó2):\n",
      "[[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "\n",
      "A has 3 columns, B has 3 rows ‚Üí Multiplication is valid ‚úì\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two matrices for demonstration\n",
    "matrix_a = np.array([[1, 2, 3],\n",
    "                     [4, 5, 6]])  # Shape: (2, 3)\n",
    "\n",
    "matrix_b = np.array([[7, 8],\n",
    "                     [9, 10],\n",
    "                     [11, 12]])   # Shape: (3, 2)\n",
    "\n",
    "print(\"Matrix A (2√ó3):\")\n",
    "print(matrix_a)\n",
    "print(f\"\\nMatrix B (3√ó2):\")\n",
    "print(matrix_b)\n",
    "print(f\"\\nA has {matrix_a.shape[1]} columns, B has {matrix_b.shape[0]} rows ‚Üí Multiplication is valid ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cf296a-14f2-4f05-95f7-1dcddbd5cdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: np.dot(matrix_a, matrix_b)\n",
      "[[ 58  64]\n",
      " [139 154]]\n",
      "Result shape: (2, 2)\n",
      "\n",
      "Calculation example for result[0,0]:\n",
      "  = (1√ó7) + (2√ó9) + (3√ó11)\n",
      "  = 7 + 18 + 33 = 58\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using np.dot() - Classic approach\n",
    "result_dot = np.dot(matrix_a, matrix_b)\n",
    "\n",
    "print(\"Method 1: np.dot(matrix_a, matrix_b)\")\n",
    "print(result_dot)\n",
    "print(f\"Result shape: {result_dot.shape}\")\n",
    "print(\"\\nCalculation example for result[0,0]:\")\n",
    "print(f\"  = (1√ó7) + (2√ó9) + (3√ó11)\")\n",
    "print(f\"  = 7 + 18 + 33 = {result_dot[0, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c267f2-7347-48a1-9b7c-47d11ed30bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 2: matrix_a @ matrix_b\n",
      "[[ 58  64]\n",
      " [139 154]]\n",
      "Result shape: (2, 2)\n",
      "\n",
      "‚úì Same result as np.dot(): True\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using @ operator - Modern, Pythonic approach (RECOMMENDED)\n",
    "result_at = matrix_a @ matrix_b\n",
    "\n",
    "print(\"Method 2: matrix_a @ matrix_b\")\n",
    "print(result_at)\n",
    "print(f\"Result shape: {result_at.shape}\")\n",
    "print(f\"\\n‚úì Same result as np.dot(): {np.array_equal(result_dot, result_at)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec10f76-92a6-4381-b6f8-c4b3e5d17450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 3: np.matmul(matrix_a, matrix_b)\n",
      "[[ 58  64]\n",
      " [139 154]]\n",
      "Result shape: (2, 2)\n",
      "\n",
      "‚úì All three methods produce identical results!\n",
      "  np.dot() == @ operator: True\n",
      "  @ operator == np.matmul(): True\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Using np.matmul() - Explicit matrix multiplication\n",
    "result_matmul = np.matmul(matrix_a, matrix_b)\n",
    "\n",
    "print(\"Method 3: np.matmul(matrix_a, matrix_b)\")\n",
    "print(result_matmul)\n",
    "print(f\"Result shape: {result_matmul.shape}\")\n",
    "print(f\"\\n‚úì All three methods produce identical results!\")\n",
    "print(f\"  np.dot() == @ operator: {np.array_equal(result_dot, result_at)}\")\n",
    "print(f\"  @ operator == np.matmul(): {np.array_equal(result_at, result_matmul)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8315da-2ea4-4cdf-9606-15be2cf59082",
   "metadata": {},
   "source": [
    "### Matrix Transpose\n",
    "\n",
    "The transpose operation is a fundamental matrix manipulation that swaps rows and columns. Understanding transposition is crucial for machine learning, as it frequently appears in gradient computations, matrix multiplications, and data transformations.\n",
    "\n",
    "#### What is Matrix Transpose?\n",
    "\n",
    "Given a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, its transpose $\\mathbf{A}^T \\in \\mathbb{R}^{n \\times m}$ is obtained by:\n",
    "- Converting rows to columns\n",
    "- Converting columns to rows\n",
    "- Element at position $(i, j)$ in $\\mathbf{A}$ moves to position $(j, i)$ in $\\mathbf{A}^T$\n",
    "\n",
    "**Mathematical Definition:**\n",
    "$$(\\mathbf{A}^T)_{ij} = \\mathbf{A}_{ji}$$\n",
    "\n",
    "#### Why Transpose Matters in Machine Learning\n",
    "\n",
    "**1. Dimension Compatibility**\n",
    "- Matrix multiplication requires: columns in first matrix = rows in second matrix\n",
    "- Transposing can make incompatible matrices compatible for multiplication\n",
    "- Example: $(m \\times n) \\times (n \\times p) = (m \\times p)$ ‚úì\n",
    "- If dimensions don't match, transpose one: $(m \\times n) \\times (k \\times p)^T = (m \\times n) \\times (p \\times k)$ when $n = p$ ‚úì\n",
    "\n",
    "**2. Gradient Computation**\n",
    "- Computing gradients often requires transposing the feature matrix\n",
    "- Example: $\\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{m} \\mathbf{X}^T (\\mathbf{Xw} - \\mathbf{y})$\n",
    "- The $\\mathbf{X}^T$ ensures proper dimension alignment for gradient calculation\n",
    "\n",
    "**3. Data Reshaping**\n",
    "- Converting between row vectors and column vectors\n",
    "- Reshaping data for different neural network architectures\n",
    "- Aligning dimensions for batch processing\n",
    "\n",
    "#### NumPy Transpose Methods\n",
    "\n",
    "NumPy provides two convenient ways to transpose matrices:\n",
    "\n",
    "**Method 1: `np.transpose()` Function**\n",
    "- Explicit function call\n",
    "- Works with multi-dimensional arrays\n",
    "- Can specify axis permutations for higher dimensions\n",
    "\n",
    "**Method 2: `.T` Attribute (RECOMMENDED)**\n",
    "- More concise and Pythonic\n",
    "- Commonly used in machine learning code\n",
    "- Mirrors mathematical notation more closely\n",
    "\n",
    "Both methods produce identical results and have the same computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a34df03d-fd38-4fe7-89d0-0d7c43dfd12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix (2√ó3):\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "Shape: (2, 3)\n",
      "\n",
      "Transposed matrix (3√ó2):\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "Shape: (3, 2)\n",
      "\n",
      "üîÑ Notice how rows become columns:\n",
      "  Original row 0: [1 2 3] ‚Üí Transposed column 0: [1 2 3]\n",
      "  Original row 1: [4 5 6] ‚Üí Transposed column 1: [4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using np.transpose() function\n",
    "\n",
    "# Define a matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6]])\n",
    "\n",
    "# Transpose the matrix\n",
    "transposed_matrix = np.transpose(matrix)\n",
    "\n",
    "print(\"Original matrix (2√ó3):\")\n",
    "print(matrix)\n",
    "print(f\"Shape: {matrix.shape}\")\n",
    "\n",
    "print(\"\\nTransposed matrix (3√ó2):\")\n",
    "print(transposed_matrix)\n",
    "print(f\"Shape: {transposed_matrix.shape}\")\n",
    "\n",
    "print(\"\\nüîÑ Notice how rows become columns:\")\n",
    "print(f\"  Original row 0: {matrix[0, :]} ‚Üí Transposed column 0: {transposed_matrix[:, 0]}\")\n",
    "print(f\"  Original row 1: {matrix[1, :]} ‚Üí Transposed column 1: {transposed_matrix[:, 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ea7662-c161-4556-ac73-737e7dc1ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Transposed matrix using .T:\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "\n",
      "‚úì Both methods produce identical results: True\n",
      "\n",
      "============================================================\n",
      "PRACTICAL EXAMPLE: Using Transpose for Matrix Multiplication\n",
      "============================================================\n",
      "\n",
      "Matrix A shape: (3, 2)\n",
      "Matrix B shape: (2, 3)\n",
      "\n",
      "A @ B possible? Columns in A (2) = Rows in B (2): True ‚úì\n",
      "Result shape: (3, 3)\n",
      "\n",
      "B @ A possible? Columns in B (3) = Rows in A (3): True ‚úì\n",
      "Result shape: (2, 2)\n",
      "\n",
      "A.T @ B possible? Columns in A.T (3) = Rows in B (2): False ‚úó\n",
      "Cannot multiply without matching dimensions!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA.T @ B possible? Columns in A.T (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) = Rows in B (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mB\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39mB\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ‚úó\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot multiply without matching dimensions!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA @ B.T shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mB\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[43mA\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ‚úì\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA.T @ B.T shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m @ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mB\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(A\u001b[38;5;241m.\u001b[39mT\u001b[38;5;250m \u001b[39m\u001b[38;5;241m@\u001b[39m\u001b[38;5;250m \u001b[39mB\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ‚úì\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 2)"
     ]
    }
   ],
   "source": [
    "# Method 2: Using the .T attribute (RECOMMENDED - More Pythonic)\n",
    "\n",
    "transposed_matrix_T = matrix.T\n",
    "\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "print(\"\\nTransposed matrix using .T:\")\n",
    "print(transposed_matrix_T)\n",
    "\n",
    "print(f\"\\n‚úì Both methods produce identical results: {np.array_equal(transposed_matrix, transposed_matrix_T)}\")\n",
    "\n",
    "# Practical example: Making matrices compatible for multiplication\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRACTICAL EXAMPLE: Using Transpose for Matrix Multiplication\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]])  # Shape: (3, 2)\n",
    "\n",
    "B = np.array([[7, 8, 9],\n",
    "              [10, 11, 12]])  # Shape: (2, 3)\n",
    "\n",
    "print(f\"\\nMatrix A shape: {A.shape}\")\n",
    "print(f\"Matrix B shape: {B.shape}\")\n",
    "\n",
    "# Can we multiply A @ B?\n",
    "print(f\"\\nA @ B possible? Columns in A ({A.shape[1]}) = Rows in B ({B.shape[0]}): {A.shape[1] == B.shape[0]} ‚úì\")\n",
    "result1 = A @ B\n",
    "print(f\"Result shape: {result1.shape}\")\n",
    "\n",
    "# What about B @ A?\n",
    "print(f\"\\nB @ A possible? Columns in B ({B.shape[1]}) = Rows in A ({A.shape[0]}): {B.shape[1] == A.shape[0]} ‚úì\")\n",
    "result2 = B @ A\n",
    "print(f\"Result shape: {result2.shape}\")\n",
    "\n",
    "# Using transpose to enable multiplication\n",
    "print(f\"\\nA.T @ B possible? Columns in A.T ({A.T.shape[1]}) = Rows in B ({B.shape[0]}): {A.T.shape[1] == B.shape[0]} ‚úó\")\n",
    "print(\"Cannot multiply without matching dimensions!\")\n",
    "\n",
    "print(f\"\\nA @ B.T shape: {A.shape} @ {B.T.shape} = {(A @ B.T).shape} ‚úì\")\n",
    "print(f\"A.T @ B.T shape: {A.T.shape} @ {B.T.shape} = {(A.T @ B.T).shape} ‚úì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e4bc0-2163-446f-aeea-239155c5d271",
   "metadata": {},
   "source": [
    "##  Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "* Master Vectorized Gradient Descent Implementation\n",
    "* Apply Scikit-learn's Closed-Form Solution \n",
    "* Implement Stochastic Gradient Descent (SGD)\n",
    "* Compare and Analyze Different Approaches\n",
    "* Build Predictive Models\n",
    "\n",
    "\n",
    "### What Makes This Lab Unique?\n",
    "\n",
    "This lab bridges the gap between **mathematical theory** and **practical implementation**. You'll see how the same linear regression problem can be solved using:\n",
    "- **Your own vectorized implementation** (deep understanding)\n",
    "- **Closed-form solution from Sklearn** (mathematical elegance)\n",
    "- **SGDRegressor from Sklean** (scalability)\n",
    "\n",
    "By comparing all three approaches, you'll develop the intuition needed to choose the right tool for any machine learning problem you encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8bc1e3-18b1-4a50-9019-730fd632c154",
   "metadata": {},
   "source": [
    "## Import necessary libaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "393fb92e-6a73-44b8-97c2-4ee7aa19345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy, math\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b972a",
   "metadata": {},
   "source": [
    "## Gradient Descent for Multiple Linear Regression using Vectorization\n",
    "\n",
    "In the previous lab, you implemented the gradient descent algorithm for Multiple Linear Regression (MLR) using **for loops**. While that approach works, it's **computationally slow** for large datasets. In this lab, you'll implement the same algorithm using **NumPy vectorization**, achieving dramatic performance improvements.\n",
    "\n",
    "### Why Vectorization?\n",
    "\n",
    "| Approach | Speed | Code Complexity | Scalability |\n",
    "|----------|-------|-----------------|-------------|\n",
    "| For Loops | Slow ‚è≥ | Higher | Poor for large data |\n",
    "| Vectorized | **Fast** ‚ö° | **Lower** | **Excellent** |\n",
    "\n",
    "\n",
    "\n",
    "### Mathematical Foundations\n",
    "\n",
    "#### Important Note on Notation\n",
    "\n",
    "**In the following equations and implementation, the weight vector $\\mathbf{w}$ incorporates the bias term $b$ as its first element.**\n",
    "\n",
    "This means:\n",
    "- $\\mathbf{w} = [b, w_1, w_2, \\ldots, w_n]^T$ (includes bias)\n",
    "- $\\mathbf{X}$ has a column of ones prepended: $\\mathbf{X}_{\\text{augmented}} = [1, x_1, x_2, \\ldots, x_n]$\n",
    "\n",
    "\n",
    "\n",
    "#### 1Ô∏è‚É£ Cost Function (Mean Squared Error)\n",
    "\n",
    "**Summation Form:**\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2 = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( e^{(i)} \\right)^2 \\tag{1}$$\n",
    "\n",
    "Where:\n",
    "- $m$ = number of training examples\n",
    "- $\\hat{y}^{(i)} = \\mathbf{x}^{(i)} \\cdot \\mathbf{w}$ = predicted value for example $i$\n",
    "- $y^{(i)}$ = actual value for example $i$\n",
    "- $e^{(i)} = \\hat{y}^{(i)} - y^{(i)}$ = prediction error for example $i$\n",
    "\n",
    "**Vectorized Form:**\n",
    "\n",
    "Define the error vector: $\\mathbf{e} = \\mathbf{Xw} - \\mathbf{y}$\n",
    "\n",
    "Then the cost becomes:\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2m} \\mathbf{e}^T \\mathbf{e} = \\frac{1}{2m} (\\mathbf{Xw} - \\mathbf{y})^T (\\mathbf{Xw} - \\mathbf{y}) \\tag{2}$$\n",
    "\n",
    "**Key Insight:** This single matrix operation replaces the entire loop over all training examples!\n",
    "\n",
    "\n",
    "\n",
    "#### 2Ô∏è‚É£ Gradient Computation\n",
    "\n",
    "The gradient tells us the direction of steepest increase in the cost function. We move in the opposite direction to minimize cost.\n",
    "\n",
    "**Vectorized Gradient:**\n",
    "$$\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = \\frac{1}{m} \\mathbf{X}^T \\mathbf{e} = \\frac{1}{m} \\mathbf{X}^T (\\mathbf{Xw} - \\mathbf{y}) \\tag{3}$$\n",
    "\n",
    "**Dimensions:**\n",
    "- $\\mathbf{X}$: $(m \\times (n+1))$ - training data with bias column\n",
    "- $\\mathbf{w}$: $((n+1) \\times 1)$ - parameters including bias\n",
    "- $\\mathbf{Xw}$: $(m \\times 1)$ - predictions for all examples\n",
    "- $\\mathbf{y}$: $(m \\times 1)$ - actual values\n",
    "- $\\mathbf{e}$: $(m \\times 1)$ - errors for all examples\n",
    "- $\\mathbf{X}^T$: $((n+1) \\times m)$ - transposed features\n",
    "- Gradient: $((n+1) \\times 1)$ - one gradient per parameter\n",
    "\n",
    "\n",
    "\n",
    "#### 3Ô∏è‚É£ Gradient Descent Update Rule\n",
    "\n",
    "Once we have the gradient, we update our parameters:\n",
    "\n",
    "$$\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} \\tag{4}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate (controls step size)\n",
    "- $:=$ means \"update to\"\n",
    "\n",
    "**This single vector operation updates ALL parameters (bias + weights) simultaneously!**\n",
    "\n",
    "\n",
    "\n",
    "###  Implementation Tasks\n",
    "\n",
    "You will implement three core functions:\n",
    "\n",
    "#### Function Pipeline\n",
    "\n",
    "```\n",
    "compute_cost_matrix(X, y, w)\n",
    "    ‚Üì (calculates current cost)\n",
    "compute_gradient_matrix(X, y, w)\n",
    "    ‚Üì (calculates direction to improve)\n",
    "gradient_descent_matrix(X, y, w_in, alpha, num_iters)\n",
    "    ‚Üì (iteratively updates parameters)\n",
    "Optimized weights w\n",
    "```\n",
    "\n",
    "**Functions to Implement:**\n",
    "\n",
    "1. **`compute_cost_matrix`** - Calculates $J(\\mathbf{w})$ using Equation (2)\n",
    "2. **`compute_gradient_matrix`** - Calculates $\\frac{\\partial J}{\\partial \\mathbf{w}}$ using Equation (3)\n",
    "3. **`gradient_descent_matrix`** - Runs the optimization loop using Equation (4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a0ab65",
   "metadata": {},
   "source": [
    "### Task 1: Implement `compute_cost_matrix` function to calculate the total cost\n",
    "\n",
    "**Hints**\n",
    "\n",
    "- Use matrix multiplication: `X @ w`\n",
    "- Compute error vector: `error = X @ w - y`\n",
    "- Use transpose: `error.T @ error` or `np.dot(error.T, error)`\n",
    "- Don't forget to divide by `2*m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the cost\n",
    "def compute_cost_matrix(X, y, w):\n",
    "    \"\"\"\n",
    "    Computes the cost for linear regression using vectorization\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n+1)): Data, m examples with n features (with bias column)\n",
    "      y (ndarray (m,))   : target values\n",
    "      w (ndarray (n+1,)) : model parameters (including bias)\n",
    "    \n",
    "    Returns:\n",
    "      cost (scalar): The cost J(w) for linear regression\n",
    "    \"\"\"\n",
    "    # Get number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # TODO: Implement the vectorized cost function\n",
    "\n",
    "    \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016b464",
   "metadata": {},
   "source": [
    "### Task 2: Implement `compute_gradient_matrix` function to calculate the gradient\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Compute predictions: `X @ w`\n",
    "- Compute errors: `X @ w - y`\n",
    "- Use transpose: `X.T @ error`\n",
    "- Divide by `m` (not `2m` for gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caeb61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the gradient\n",
    "def compute_gradient_matrix(X, y, w):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression using vectorization\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n+1)): Data, m examples with n features (with bias column)\n",
    "      y (ndarray (m,))   : target values\n",
    "      w (ndarray (n+1,)) : model parameters (including bias)\n",
    "    \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n+1,)): The gradient of the cost w.r.t. the parameters w\n",
    "    \"\"\"\n",
    "    # Get number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # TODO: Implement the vectorized gradient computation\n",
    "\n",
    "    \n",
    "    \n",
    "    return dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb5aa7",
   "metadata": {},
   "source": [
    "### Task 3: Implement `gradient_descent_matrix` function below\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Initialize cost history: `J_history = []`\n",
    "- Loop for `num_iters` iterations\n",
    "- Update weights: `w = w - alpha * gradient`\n",
    "- Track cost every iteration (or every 100 iterations)\n",
    "- Print progress to monitor convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473c94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform gradient descent\n",
    "def gradient_descent_matrix(X, y, w_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w using vectorization.\n",
    "    Updates w by taking num_iters gradient steps with learning rate alpha.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n+1))   : Data, m examples with n features (with bias column)\n",
    "      y (ndarray (m,))      : target values\n",
    "      w_in (ndarray (n+1,)) : initial model parameters (including bias)\n",
    "      alpha (float)         : Learning rate\n",
    "      num_iters (int)       : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n+1,))    : Updated values of parameters after training\n",
    "      J_history (list)      : History of cost values at each iteration\n",
    "    \"\"\"\n",
    "    # TODO: Implement the batch gradient descent algorithm\n",
    "    # Hint: Follow these steps:\n",
    "    # 1. Initialize J_history as an empty list\n",
    "    # 2. Make a copy of w_in to avoid modifying the original: w = copy.deepcopy(w_in)\n",
    "    # 3. Loop for num_iters iterations:\n",
    "    #    a. Calculate gradient using compute_gradient_matrix(X, y, w)\n",
    "    #    b. Update w: w = w - alpha * gradient\n",
    "    #    c. Calculate cost using compute_cost_matrix(X, y, w)\n",
    "    #    d. Append cost to J_history\n",
    "    #    e. Print cost every 100 iterations (or every math.ceil(num_iters/10) iterations)\n",
    "    # 4. Return w and J_history\n",
    "    \n",
    "    \n",
    "    return w, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7b9b7",
   "metadata": {},
   "source": [
    "### Task 4: Check your implementation on the house dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7276658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = np.loadtxt(\"data/houses.txt\", delimiter=',')\n",
    "X_train, y_train = data[:,:-1], data[:,-1]\n",
    "X_features = ['size(sqft)','bedrooms','floors','age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c34d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do feature scaling for the x_train below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of ones to X_train to account for the bias term\n",
    "# Hint: Use np.c_[ones_column, X_train] or np.hstack()\n",
    "# The ones column should have shape (m, 1) where m = number of examples\n",
    "\n",
    "\n",
    "# Initialize the model parameters to 0\n",
    "# Hint: w should have shape (n+1,) where n+1 includes bias + features\n",
    "# Use np.zeros()\n",
    "\n",
    "\n",
    "# Run the gradient descent algorithm\n",
    "# Set: iterations = 1000, learning rate = 0.01\n",
    "\n",
    "\n",
    "# Print the optimized weights\n",
    "print(\"Optimized weights (w):\")\n",
    "print(f\"  Bias (w[0]): {w[0]:.2f}\")\n",
    "for i in range(1, len(w)):\n",
    "    print(f\"  {X_features[i-1]} (w[{i}]): {w[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d5f12c",
   "metadata": {},
   "source": [
    "### Task 5: Plotting the cost over the iterations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c2a7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cost over the iterations below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623c38d-af0e-4400-a5a9-ca9af2fdaae6",
   "metadata": {},
   "source": [
    "## Scikit-learn: Linear Regression with Closed-Form Solution\n",
    "\n",
    "### Overview\n",
    "\n",
    "Scikit-learn provides the [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class, which implements a **closed-form solution** for linear regression using the **Normal Equation**. This approach computes the optimal weights directly through matrix algebra, without requiring iterative optimization.\n",
    "\n",
    "### Mathematical Foundation: The Normal Equation\n",
    "\n",
    "The closed-form solution derives from calculus by setting the gradient of the cost function to zero and solving for $\\mathbf{w}$:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "**Key Characteristics:**\n",
    "\n",
    "- ‚úÖ **No iterations required** - Computes optimal weights in one step\n",
    "- ‚úÖ **No learning rate needed** - No hyperparameters to tune\n",
    "- ‚úÖ **Exact solution** - Finds global minimum directly (no convergence issues)\n",
    "- ‚úÖ **No feature scaling required** - Works with raw features\n",
    "\n",
    "### When to Use Closed-Form vs. Gradient Descent\n",
    "\n",
    "| Factor | Closed-Form Solution | Gradient Descent |\n",
    "|--------|---------------------|------------------|\n",
    "| **Dataset Size** | Small to medium (< 10,000 examples) | Large datasets (millions of examples) |\n",
    "| **Computation** | $O(n^3)$ due to matrix inversion | $O(kn)$ where k = iterations |\n",
    "| **Speed** | Fast for small $n$ | Faster for large $n$ |\n",
    "| **Scalability** | Poor - requires inverting $\\mathbf{X}^T \\mathbf{X}$ | Excellent - iterative updates |\n",
    "| **Feature Scaling** | **Not required** ‚úì | Required for good performance |\n",
    "| **Hyperparameters** | None | Learning rate, iterations |\n",
    "| **Convergence** | Always finds exact solution | May need tuning for convergence |\n",
    "\n",
    "### Important Computational Considerations\n",
    "\n",
    "**Matrix Inversion Complexity:**\n",
    "\n",
    "- The Normal Equation requires computing $(\\mathbf{X}^T \\mathbf{X})^{-1}$\n",
    "- For $n$ features, this is an $(n \\times n)$ matrix\n",
    "- Matrix inversion has computational complexity of $O(n^3)$\n",
    "- **Becomes impractical when $n > 10,000$ features**\n",
    "\n",
    "**Memory Requirements:**\n",
    "\n",
    "- Must store $\\mathbf{X}^T \\mathbf{X}$ matrix in memory\n",
    "- For 10,000 features: $(10^4)^2 = 100$ million elements\n",
    "- Can cause memory issues on large datasets\n",
    "\n",
    "### When Closed-Form Solution Works Best\n",
    "\n",
    "‚úÖ **Ideal Scenarios:**\n",
    "\n",
    "- Small to medium datasets (like our house price example)\n",
    "- Number of features < 1,000\n",
    "- Need quick, exact solution without tuning\n",
    "- Educational purposes to understand optimal parameters\n",
    "- Baseline model for comparison\n",
    "\n",
    "‚ùå **Avoid When:**\n",
    "\n",
    "- Large datasets (> 100,000 examples)\n",
    "- High-dimensional data (> 10,000 features)\n",
    "- Real-time or streaming data scenarios\n",
    "- Computational resources are limited\n",
    "\n",
    "### Important Note on Feature Scaling\n",
    "\n",
    "> ‚ö†Ô∏è **The closed-form solution does NOT require feature normalization.**\n",
    ">\n",
    "> Unlike gradient descent, which converges faster with normalized features, the Normal Equation produces the same optimal weights regardless of feature scales. This is because it solves the optimization problem algebraically rather than iteratively.\n",
    "\n",
    "### Implementation in Scikit-learn\n",
    "\n",
    "In Task 6 below, you'll use `LinearRegression` to:\n",
    "\n",
    "1. Fit the model using the Normal Equation\n",
    "2. Extract learned parameters (bias and weights)\n",
    "3. Compare results with your gradient descent implementation\n",
    "4. Verify that different methods converge to the same solution\n",
    "\n",
    "**Expected Outcome:**\n",
    "\n",
    "The weights learned from `LinearRegression` should be very close to those from your gradient descent implementation, validating both approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4ee21",
   "metadata": {},
   "source": [
    "### Task 6: Using linear regression from sklearn to learn the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a697131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use linear regression from sklearn to learn the model parameters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e09293-5da2-4d3c-af8f-4284fab528be",
   "metadata": {},
   "source": [
    "## Scikit-learn: Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### Overview\n",
    "\n",
    "Scikit-learn provides the [`SGDRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) class, which implements **Stochastic Gradient Descent** for linear regression. Unlike batch gradient descent (which uses all training examples in each iteration) or the closed-form solution (which requires expensive matrix inversion), SGD updates weights using **one or a few examples at a time**, making it highly scalable for large datasets.\n",
    "\n",
    "### What is Stochastic Gradient Descent?\n",
    "\n",
    "**Batch Gradient Descent** (what you implemented):\n",
    "$$\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\nabla J^{(i)}(\\mathbf{w})$$\n",
    "\n",
    "- Uses **all** $m$ training examples per update\n",
    "- Smooth, consistent convergence\n",
    "- Computationally expensive for large datasets\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)**:\n",
    "$$\\mathbf{w} := \\mathbf{w} - \\alpha \\nabla J^{(i)}(\\mathbf{w})$$\n",
    "\n",
    "- Uses **one** randomly selected example per update\n",
    "- Much faster iterations\n",
    "- Noisy convergence path, but reaches good solution quickly\n",
    "\n",
    "**Mini-Batch Gradient Descent** (what SGDRegressor typically uses):\n",
    "$$\\mathbf{w} := \\mathbf{w} - \\alpha \\frac{1}{b} \\sum_{i=1}^{b} \\nabla J^{(i)}(\\mathbf{w})$$\n",
    "\n",
    "- Uses **small batch** of $b$ examples per update (e.g., $b = 32, 64, 128$)\n",
    "- Balances speed and stability\n",
    "- Best of both worlds for most applications\n",
    "\n",
    "### Key Differences: Batch GD vs. SGD\n",
    "\n",
    "| Aspect | Batch Gradient Descent | Stochastic Gradient Descent |\n",
    "|--------|------------------------|----------------------------|\n",
    "| **Examples per Update** | All $m$ examples | 1 or small batch ($b$) |\n",
    "| **Iteration Speed** | Slow for large data | Very fast ‚ö° |\n",
    "| **Convergence Path** | Smooth, direct | Noisy, oscillating |\n",
    "| **Memory Usage** | High (loads all data) | Low (loads small batches) |\n",
    "| **Scalability** | Poor for large datasets | **Excellent** üöÄ |\n",
    "| **Online Learning** | Not suitable | **Perfect for streaming data** |\n",
    "| **Feature Scaling** | Helpful | **Critical** ‚ö†Ô∏è |\n",
    "| **Final Accuracy** | Exact minimum | Close to minimum (with tuning) |\n",
    "\n",
    "### Why Feature Scaling is Critical for SGD\n",
    "\n",
    "> ‚ö†Ô∏è **The SGD solution performs best with normalized inputs.**\n",
    "\n",
    "**Reason**: SGD uses a **single learning rate** $\\alpha$ for all features. Without scaling:\n",
    "\n",
    "- Features with large ranges (e.g., house size: 500-5000 sqft) cause large gradients\n",
    "- Features with small ranges (e.g., bedrooms: 1-5) cause small gradients\n",
    "- One learning rate cannot work well for both ‚Üí slow or unstable convergence\n",
    "\n",
    "**With Feature Scaling (StandardScaler)**:\n",
    "\n",
    "- All features have similar ranges (typically mean=0, std=1)\n",
    "- Gradients are balanced across all features\n",
    "- Learning rate works uniformly ‚Üí **fast, stable convergence** ‚úì\n",
    "\n",
    "**Scaling Methods:**\n",
    "\n",
    "```python\n",
    "# StandardScaler: (x - mean) / std_dev ‚Üí mean=0, std=1\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "```\n",
    "\n",
    "### When to Use SGD vs. Other Methods\n",
    "\n",
    "‚úÖ **Use SGD When:**\n",
    "\n",
    "- **Large datasets** (> 100,000 examples)\n",
    "- **Online/streaming data** (model updates as new data arrives)\n",
    "- **Limited memory** (cannot load entire dataset)\n",
    "- **High-dimensional data** (many features)\n",
    "- **Quick approximate solution** needed\n",
    "- **Scalability** is critical\n",
    "\n",
    "‚ùå **Avoid SGD When:**\n",
    "\n",
    "- Small datasets (< 1,000 examples) ‚Üí use closed-form solution\n",
    "- Need exact solution ‚Üí use Normal Equation\n",
    "- Cannot scale features properly\n",
    "- Convergence stability is critical\n",
    "\n",
    "### SGD in Practice: Real-World Applications\n",
    "\n",
    "**1. Large-Scale Machine Learning**\n",
    "\n",
    "- Training on millions/billions of examples\n",
    "- Deep learning models (neural networks use variants of SGD)\n",
    "- Recommendation systems with huge user bases\n",
    "\n",
    "**2. Online Learning**\n",
    "\n",
    "- Stock price prediction (continuous data stream)\n",
    "- Click-through rate prediction\n",
    "- Real-time fraud detection\n",
    "\n",
    "**3. Limited Resources**\n",
    "\n",
    "- Mobile/edge devices with memory constraints\n",
    "- Distributed computing environments\n",
    "- Cloud computing cost optimization\n",
    "\n",
    "\n",
    "### Implementation in Scikit-learn\n",
    "\n",
    "In Task 7 below, you'll use `SGDRegressor` to:\n",
    "\n",
    "1. **Scale your features** using `StandardScaler` (critical step!)\n",
    "2. Fit the model using SGD optimization\n",
    "3. Extract learned parameters (bias and weights)\n",
    "4. Compare convergence speed and final results with other methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f279f0",
   "metadata": {},
   "source": [
    "### Task 7: Using stochastic gradient descent from sklearn to learn the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stochastic gradient descent from sklearn to learn the model parameters \n",
    "# use compariable hyperparameters as gradient descent above\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d8ec2",
   "metadata": {},
   "source": [
    "**Expected Outcome:**\n",
    "With proper feature scaling, SGD should converge quickly to weights very similar to batch gradient descent and the closed-form solution, demonstrating that all three methods solve the same optimization problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fcaf9",
   "metadata": {},
   "source": [
    "## Comparative Analysis: Putting It All Together\n",
    "\n",
    "### Overview\n",
    "\n",
    "Now that you've implemented three different approaches to solving the same linear regression problem, it's time to **systematically compare** the results. This comparison will reveal important insights about the relationship between different optimization methods and validate your implementations.\n",
    "\n",
    "### What You've Accomplished So Far\n",
    "\n",
    "By this point, you should have trained three models:\n",
    "\n",
    "**1. Your Vectorized Gradient Descent Implementation** \n",
    "\n",
    "- ‚úÖ Implemented from scratch using NumPy\n",
    "- ‚úÖ Used batch gradient descent with all training examples\n",
    "- ‚úÖ Applied feature scaling for faster convergence\n",
    "- ‚úÖ Iterative optimization approach\n",
    "\n",
    "**2. Scikit-learn's LinearRegression (Closed-Form Solution)** \n",
    "\n",
    "- ‚úÖ Used the Normal Equation: $\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$\n",
    "- ‚úÖ Direct computation without iterations\n",
    "- ‚úÖ No feature scaling required\n",
    "- ‚úÖ Exact mathematical solution\n",
    "\n",
    "**3. Scikit-learn's SGDRegressor (Stochastic Gradient Descent)**\n",
    " \n",
    "- ‚úÖ Used mini-batch gradient descent\n",
    "- ‚úÖ Applied feature scaling (critical for SGD)\n",
    "- ‚úÖ Iterative optimization with random sampling\n",
    "- ‚úÖ Scalable approach for large datasets\n",
    "\n",
    "\n",
    "This comparison is the **culminating moment** where theory meets practice and validates your understanding of linear regression optimization! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba9f255",
   "metadata": {},
   "source": [
    "### Task 8: Put the model parameters from gradient descent, linear regression and SGD in a dataframe for comparision\n",
    "\n",
    "\n",
    "You'll create a **comparison table (DataFrame)** showing:\n",
    "\n",
    "- **Bias term** from each method\n",
    "- **Weights for each feature** from each method\n",
    "- Side-by-side comparison for easy visual inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef743cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the model parameters from gradient descent, linear regression and SGD in a dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa36d0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Example Output Format:**\n",
    "```\n",
    "                    Gradient Descent    Linear Regression    SGD Regressor\n",
    "Bias                     -23.45              -23.47             -23.42\n",
    "size(sqft)                45.67               45.68              45.65\n",
    "bedrooms                  12.34               12.35              12.33\n",
    "floors                     8.91                8.92               8.90\n",
    "age                       -5.67               -5.68              -5.66\n",
    "```\n",
    "Success Criteria ‚úì\n",
    "\n",
    "Your comparison is successful if:\n",
    "\n",
    "- All three methods produce weights with **matching signs** (all positive or all negative for each feature)\n",
    "- Numerical values are **within 5% of each other** (preferably < 1%)\n",
    "- The **bias terms** are similar across all methods\n",
    "- Results make **intuitive sense** (e.g., larger houses cost more ‚Üí positive weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e4dd3",
   "metadata": {},
   "source": [
    "##  Making Predictions: Applying Your Trained Models\n",
    "\n",
    "### Overview\n",
    "\n",
    "The ultimate goal of any machine learning model is to **make accurate predictions on new, unseen data**. In this final task, you'll use all three trained models to predict house prices for a new property, demonstrating the complete end-to-end machine learning workflow.\n",
    "\n",
    "\n",
    "### The Prediction Scenario\n",
    "\n",
    "**New House Features:**\n",
    "\n",
    "- **Size**: 1,650 sqft\n",
    "- **Bedrooms**: 3\n",
    "- **Floors**: 2\n",
    "- **Age**: 20 years old\n",
    "\n",
    "**Your Task:** Predict the price using all three models and compare the results.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "For a new example $\\mathbf{x}_{\\text{new}}$, the prediction is:\n",
    "\n",
    "$$\\hat{y}_{\\text{new}} = \\mathbf{w}^T \\mathbf{x}_{\\text{new}} = b + w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4$$\n",
    "\n",
    "Or in vectorized form (with bias included in $\\mathbf{w}$):\n",
    "$$\\hat{y}_{\\text{new}} = \\mathbf{w}^T \\mathbf{x}_{\\text{new}} = [b, w_1, w_2, w_3, w_4] \\cdot [1, x_1, x_2, x_3, x_4]$$\n",
    "\n",
    "### Critical: Feature Scaling for Predictions ‚ö†Ô∏è\n",
    "\n",
    "**The Most Common Mistake in ML:**\n",
    "\n",
    "```python\n",
    "# ‚ùå WRONG - Using raw features\n",
    "x_new = [1650, 3, 2, 20]\n",
    "prediction = model.predict([x_new])  # Will give wrong result!\n",
    "\n",
    "# ‚úì CORRECT - Scaling new features first\n",
    "x_new = [1650, 3, 2, 20]\n",
    "x_new_scaled = scaler.transform([x_new])  # Use same scaler from training\n",
    "prediction = model.predict(x_new_scaled)  # Correct prediction\n",
    "```\n",
    "\n",
    "**Why Scaling Matters:**\n",
    "\n",
    "- Your models were trained on **scaled features** (mean=0, std=1)\n",
    "- New data must be transformed using the **same scaling parameters**\n",
    "- Use `scaler.transform()` (NOT `fit_transform()`) on new data\n",
    "- `fit_transform()` would compute NEW scaling parameters ‚Üí wrong predictions!\n",
    "\n",
    "### Different Approaches for Each Model\n",
    "\n",
    "**1. Your Gradient Descent Model:**\n",
    "\n",
    "```python\n",
    "# Manually create feature vector with bias term\n",
    "x_new = np.array([1, x1_scaled, x2_scaled, x3_scaled, x4_scaled])\n",
    "prediction = np.dot(w, x_new)  # or w @ x_new\n",
    "```\n",
    "\n",
    "**2. LinearRegression (sklearn):**\n",
    "```python\n",
    "# Use .predict() method (handles bias automatically)\n",
    "x_new_scaled = scaler.transform([[1650, 3, 2, 20]])\n",
    "prediction = lr_model.predict(x_new_scaled)\n",
    "```\n",
    "\n",
    "**3. SGDRegressor (sklearn):**\n",
    "```python\n",
    "# Use .predict() method (handles bias automatically)\n",
    "x_new_scaled = scaler.transform([[1650, 3, 2, 20]])\n",
    "prediction = sgd_model.predict(x_new_scaled)\n",
    "```\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "**What You Should See:**\n",
    "\n",
    "All three predictions should be **very close** to each other:\n",
    "```\n",
    "Gradient Descent Prediction: $292,450\n",
    "Linear Regression Prediction: $292,470\n",
    "SGD Regressor Prediction:     $292,430\n",
    "```\n",
    "\n",
    "**Success Criteria:**\n",
    "\n",
    "- All predictions within **2-3% of each other** ‚úì\n",
    "- Predictions make **intuitive sense** (reasonable house price) ‚úì\n",
    "- No negative prices or extreme outliers ‚úì\n",
    "\n",
    "### What If Predictions Differ Significantly?\n",
    "\n",
    "**If predictions vary by more than 5%, check:**\n",
    "\n",
    "1. **Feature Scaling Issues**\n",
    "   \n",
    "   - Did you scale the new features using the same scaler?\n",
    "   - Are you using `transform()` not `fit_transform()`?\n",
    "\n",
    "2. **Bias Term Handling**\n",
    "   \n",
    "   - For your GD model: Did you prepend `1` for the bias?\n",
    "   - For sklearn models: They handle bias automatically\n",
    "\n",
    "3. **Model Convergence**\n",
    "   \n",
    "   - Did your gradient descent fully converge?\n",
    "   - Check the cost plot - should flatten out\n",
    "\n",
    "4. **Parameter Extraction**\n",
    "   \n",
    "   - For sklearn models: Using `intercept_` and `coef_` correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cfc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your prediction code heretion code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d66c5-e659-4429-9f43-eec667177271",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "In this lab, you:\n",
    "\n",
    "- Implemented gradient descent for linear regression using vectorization, gaining insights into efficient computation.\n",
    "- Utilized scikit-learn to implement linear regression with a closed-form solution based on the normal equation, exploring its simplicity and precision.\n",
    "- Applied scikit-learn to perform linear regression using stochastic gradient descent, understanding its flexibility and scalability.\n",
    "- Compared the parameters learned from different approaches, deepening your understanding of their strengths and trade-offs.\n",
    "- Made predictions on new instances using each approach.\n",
    "\n",
    "Great work‚Äîyou now have a confident grasp of vectorized gradient descent and how to choose the right training strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f49bbb-7891-4d09-b763-a9d3685e6d5c",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "https://www.deeplearning.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ff757-7db9-4286-bfdd-1b654aa39b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
