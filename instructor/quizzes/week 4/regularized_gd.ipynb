{
 "cells": [
  {
   "cell_type": "raw",
   "id": "923aee8b-9063-41e2-8387-ccb34025dc70",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Quiz 5: Regularized Gradient Descent for Linear Regression\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    number-sections: true\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cdb3da",
   "metadata": {},
   "source": [
    "##  Learning Objectives\n",
    "\n",
    "By the end of this comprehensive lab, you will master the following key concepts and skills:\n",
    "\n",
    "### Theoretical Understanding\n",
    "* **Grasp the overfitting problem**: Understand how complex models can memorize training data but fail to generalize\n",
    "* **Learn regularization fundamentals**: Discover how L1 and L2 penalties constrain model complexity\n",
    "* **Mathematical foundations**: Derive and understand the modified cost functions and gradient formulas for regularized regression\n",
    "\n",
    "### Implementation Mastery\n",
    "* **Extend cost functions**: Add L1 (Lasso) and L2 (Ridge) regularization terms to prevent overfitting\n",
    "* **Vectorized gradient descent**: Implement efficient, matrix-based gradient descent algorithms for both regularization types  \n",
    "* **Handle non-differentiability**: Navigate the challenges of L1 regularization's non-smooth optimization landscape\n",
    "* **Professional coding practices**: Write clean, documented, and efficient machine learning code\n",
    "\n",
    "### Practical Application\n",
    "* **Real overfitting scenario**: Work with a deliberately overfitted polynomial regression model\n",
    "* **Hyperparameter tuning**: Explore how regularization strength (λ) affects model behavior\n",
    "* **Scikit-learn integration**: Leverage industry-standard Lasso and Ridge implementations\n",
    "* **Performance evaluation**: Use RMSE and learning curves to assess model quality\n",
    "\n",
    "### Comparative Analysis\n",
    "* **Method comparison**: Systematically compare custom implementations vs. scikit-learn\n",
    "* **L1 vs L2 analysis**: Understand when to use Lasso (feature selection) vs Ridge (parameter shrinkage)\n",
    "* **Bias-variance tradeoff**: Observe how regularization balances underfitting and overfitting\n",
    "* **Convergence behavior**: Analyze training vs. test performance across different approaches\n",
    "\n",
    "### Key Insights You'll Gain\n",
    "* Why regularization is essential for small datasets and high-dimensional features\n",
    "* How L1 regularization promotes sparsity (automatic feature selection)\n",
    "* Why L2 regularization provides smoother, more stable solutions\n",
    "* When to choose custom implementations vs. pre-built libraries in real-world projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a58cd4",
   "metadata": {},
   "source": [
    "## Essential Libraries and Dependencies\n",
    "\n",
    "We'll import the core libraries needed for this comprehensive regularization lab:\n",
    "\n",
    "- **NumPy & Matplotlib**: Numerical computing and data visualization\n",
    "- **Pandas**: Data manipulation and analysis framework\n",
    "- **Scikit-learn**: Professional ML implementations (Lasso, Ridge, StandardScaler)\n",
    "- **Math & Copy**: Utility functions for mathematical operations and object copying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393fb92e-6a73-44b8-97c2-4ee7aa19345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy, math\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f3d2b",
   "metadata": {},
   "source": [
    "## Synthetic Dataset Creation: Setting Up for Overfitting\n",
    "\n",
    "**Educational Purpose**: We'll deliberately create a scenario prone to overfitting:\n",
    "\n",
    "- **Small sample size** (only ~10% for training)\n",
    "- **High-dimensional features** (polynomial terms up to x^15)\n",
    "- **Noisy target function** (sine wave + random noise)\n",
    "\n",
    "This setup perfectly demonstrates why regularization is essential in machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input array with angles from 60deg to 300deg converted to radians\n",
    "x = np.array([i*np.pi/60 for i in range(120)])\n",
    "# x = np.array([i*np.pi/180 for i in range(360)])\n",
    "\n",
    "np.random.seed(10)  #Setting seed for reproducibility\n",
    "y = np.sin(x) + np.random.normal(0,0.15,len(x))\n",
    "data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])\n",
    "plt.plot(data['x'],data['y'],'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df268c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,16):  #power of 1 is already there\n",
    "    colname = 'x_%d'%i      #new var will be x_power\n",
    "    data[colname] = data['x']**i\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test \n",
    "np.random.seed(123)\n",
    "# split the data into train and test, less data is prone to overfitting, so we will only use  10% data for training\n",
    "mask = np.random.rand(len(data)) < 0.1\n",
    "train = data[mask]\n",
    "test = data[~mask]\n",
    "print('Number of observations in the training data:', len(train))\n",
    "print('Number of observations in the test data:',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split feature and target\n",
    "X_train = train.drop('y', axis=1).values\n",
    "y_train = train['y'].values\n",
    "X_test = test.drop('y', axis=1).values\n",
    "y_test = test['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1222509",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[:,0], y_train, 'b.')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a4123d",
   "metadata": {},
   "source": [
    "## Critical Observation: The Perfect Storm for Overfitting\n",
    "\n",
    "This is a textbook case where regularization becomes absolutely essential!\n",
    "\n",
    "Notice the dangerous combination we've created:\n",
    "\n",
    "- **Extremely small training set**: Only ~7-12 training instances\n",
    "   - No regularization means unlimited parameter growth\n",
    "\n",
    "\n",
    "- **High feature dimensionality**: 15 polynomial features  \n",
    "   - Each polynomial term can memorize individual training points\n",
    "\n",
    "- **Complex underlying pattern**: Sine wave with noise\n",
    "   \n",
    "\n",
    "**Why this leads to overfitting:**\n",
    "\n",
    "- The model has more parameters (15) than training examples (~7-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f67140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e68724f",
   "metadata": {},
   "source": [
    "## Baseline Implementation: Vanilla Gradient Descent\n",
    "\n",
    "### Prerequisites: Building on Previous Knowledge\n",
    "\n",
    "Before we add regularization, let's establish our baseline with standard gradient descent. \n",
    "\n",
    "**Key Requirement**: Modified to track both training AND test performance to demonstrate overfitting.\n",
    "\n",
    "**Your Task**: Implement the core gradient descent functions from your previous lab:\n",
    "\n",
    "- `compute_cost_matrix()`: Calculate MSE cost using vectorization\n",
    "- `gradient_descent_matrix()`: Main optimization loop\n",
    "- `compute_gradient_matrix()`: Compute gradients efficiently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bd94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_matrix(X, y, w):\n",
    "    \"\"\"\n",
    "    Computes the Mean Squared Error cost for linear regression using vectorization\n",
    "    \n",
    "    Mathematical Formula: J(w) = (1/2m) * ||Xw - y||²\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n+1)): Feature matrix, m training examples with n+1 features (including bias)\n",
    "      y (ndarray (m,)): Target values for training examples\n",
    "      w (ndarray (n+1,)): Model parameters (weights + bias)\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): Mean squared error cost\n",
    "      \n",
    "\n",
    "    Implementation Hint:    return cost\n",
    "\n",
    "      Use vectorized operations: (X @ w - y).T @ (X @ w - y) / (2*m)    \n",
    "\n",
    "    \"\"\"    # Formula: J = (1/2m) * (predictions - targets)^2\n",
    "    # TODO: Implement vectorized MSE cost calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_matrix(X, y, w):\n",
    "    \"\"\"\n",
    "    Computes the gradient of MSE cost w.r.t. parameters using vectorization\n",
    "    \n",
    "    Mathematical Formula: ∇J(w) = (1/m) * X.T @ (Xw - y)\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n+1)): Feature matrix with bias column\n",
    "      y (ndarray (m,)): Target values \n",
    "      w (ndarray (n+1,)): Current model parameters\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n+1,)): Gradient vector for all parameters\n",
    "      \n",
    "    Implementation Hint:\n",
    "\n",
    "      Vectorized form avoids loops: X.T @ (predictions - targets) / m    return dj_dw\n",
    "\n",
    "    \"\"\"    \n",
    "\n",
    "    # TODO: Implement vectorized gradient calculation    # Formula: dJ/dw = (1/m) * X.T * (X*w - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44b134",
   "metadata": {},
   "source": [
    "## Enhanced Gradient Descent: Tracking Overfitting in Real-Time\n",
    "\n",
    "- Return both cost histories for visualization\n",
    "\n",
    "**Critical Modification**: We need to monitor BOTH training and test performance simultaneously to observe overfitting as it happens.\n",
    "\n",
    "- Print progress every 1000 iterations for monitoring\n",
    "- Calculate test cost at each iteration (without updating weights on test data)\n",
    "\n",
    "**Key Requirements:**\n",
    "\n",
    "- Track training cost at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_matrix(X, y, X_test, y_test, w_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w. Updates w by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (?,n+1))   : Training set, ? examples with n features\n",
    "      y (ndarray (?,))    : target values in test set\n",
    "      X_test (ndarray (?,n+1))   : Test set, ? examples with n features\n",
    "      y_test (ndarray (?,))    : target values in test set\n",
    "      w_in (ndarray (n+1,)) : initial model parameters  \n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n+1,)) : Updated values of parameters\n",
    "      J_history           : Cost of the model on the training set after each iteration\n",
    "      test_J_history      : Cost of the model on test set after each iteration\n",
    "      \"\"\"\n",
    "    \n",
    "    # implement the batch gradient descent algorithm using the compute_gradient and compute_cost functions\n",
    "    \n",
    "    return w, J_history, test_J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c176ebc",
   "metadata": {},
   "source": [
    "## Testing Your Implementation: Demonstrating Overfitting\n",
    "\n",
    "**Expected Outcome**: You should observe the training cost decreasing while test cost increases - classic overfitting!\n",
    "\n",
    "**Experiment Parameters:**\n",
    "\n",
    "- Learning rate: `alpha = 0.1` (aggressive for faster convergence)\n",
    "- Data: Use standardized features (`X_train_scaled`, `X_test_scaled`)\n",
    "- Iterations: `num_iters = 50000` (ensure full convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a34a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the gradient descent with the scaled data\n",
    "# add a column of ones to the X_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55dfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the learning curve to show the overfitting issue of the model\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].plot(J_history, label='Training')\n",
    "axs[0].plot(test_J_history, label='Test')\n",
    "axs[0].set_xlabel('Iterations')\n",
    "axs[0].set_ylabel('Cost')\n",
    "axs[0].set_title('Cost vs. Iterations')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(J_history[5000:], label='Training')\n",
    "axs[1].plot(test_J_history[5000:], label='Test')\n",
    "axs[1].set_xlabel('Iterations')\n",
    "axs[1].set_ylabel('Cost')\n",
    "axs[1].set_title('Cost vs. Iterations after 5000 iterations')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a719dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model on the train and test data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "fig.suptitle('Train and Test Data')\n",
    "axs[0].plot(X_train[:, 0:1], X_train_scaled_1 @ w, 'r')\n",
    "axs[0].plot(X_train[:, 0:1], y_train, '.')\n",
    "axs[0].set_title('Train Data')\n",
    "axs[1].plot(X_test[:, 0:1], X_test_scaled_1 @ w, 'g')\n",
    "axs[1].plot(X_test[:, 0:1], y_test, '.')\n",
    "axs[1].set_title('Test Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the final RMSE for the training and test set\n",
    "gd_rmse_train = math.sqrt(J_history[-1])\n",
    "gd_rmse_test = math.sqrt(test_J_history[-1])\n",
    "print(\"Final training RMSE: \",gd_rmse_train)\n",
    "print(\"Final test RMSE: \", gd_rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802e2472",
   "metadata": {},
   "source": [
    "## Analysis: Overfitting Confirmed!\n",
    "\n",
    "**The Solution**: Add regularization to constrain parameter growth and improve generalization. \n",
    "\n",
    "**What You Should Observe:**\n",
    "\n",
    "- **Training RMSE**: Very low (model memorized training data)\n",
    "  - **The Problem**: Our model learned the noise, not the underlying pattern!\n",
    "\n",
    "- **Test RMSE**: Much higher (poor generalization)\n",
    "\n",
    "- **Learning curves**: Training cost ↓, Test cost ↑ (diverging lines)\n",
    "- **Visualizations**: Perfect fit on training, erratic on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4576e7ca",
   "metadata": {},
   "source": [
    "## Regularization: The Overfitting Solution\n",
    "\n",
    "**Core Concept**: Add a penalty term to the cost function that discourages large parameter values.\n",
    "\n",
    "**Why Regularization Works:**\n",
    "- **Constrains model complexity** by limiting parameter magnitude\n",
    "- **Improves generalization** by preventing memorization of training noise\n",
    "- **Provides smoothness** by preferring simpler, more stable solutions\n",
    "- **Reduces variance** while introducing controlled bias\n",
    "\n",
    "**Two Main Regularization Types:**\n",
    "- **L1 (Lasso)**: λ||w||₁ - Promotes sparsity, drives weights to exactly zero\n",
    "- **L2 (Ridge)**: λ||w||₂² - Shrinks weights smoothly, keeps all features active\n",
    "\n",
    "**Mathematical Intuition**: Both add penalty terms that grow with parameter magnitude, forcing the optimizer to balance fitting the data with keeping weights small.\n",
    "\n",
    "<b>Cost function for L1 regularized linear regression</b>\n",
    "\\begin{equation}\n",
    "J(w) = 1/2m \\sum_{i=1}^{m} (f_{\\mathbf{w}}(\\mathbf{x})^{(i)} - y^{(i)})^2 +\\lambda/2m\\sum_{j=1}^{n} |w_j|  \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "**Vectorized L1 Regularized Cost Function with Summation**\n",
    "\n",
    "\\begin{equation}\n",
    "J(W) = \\frac{1}{2m} (XW - y)^T (XW - y) + \\lambda \\sum_{j=1}^{n} |w_j|   \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "**L1 In Vectorized Form**\n",
    "\n",
    "\\begin{equation}\n",
    "J(W) = \\frac{1}{2m} (XW - y)^T (XW - y) + \\lambda \\| W \\|_1   \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "<b>Cost function for L2 regularized linear regression</b>\n",
    "\n",
    "\\begin{equation}\n",
    "J(w) = 1/2m \\sum_{i=1}^{m} (f_{\\mathbf{w}}(\\mathbf{x})^{(i)} - y^{(i)})^2 +\\lambda/2m\\sum_{j=1}^{n} w_j^2   \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Vectorized L2 Regularized Cost Function with Summation**\n",
    "\n",
    "\\begin{equation}\n",
    "J(W) = \\frac{1}{2m} (XW - y)^T (XW - y) + \\lambda \\sum_{j=1}^{n} w_j^2    \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "**L2 Regularized Cost Function in Vectorized Form**\n",
    "\n",
    "\\begin{equation}\n",
    "J(W) = \\frac{1}{2m} (XW - y)^T (XW - y) + \\frac{\\lambda}{2} \\| W \\|_2^2    \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Compare this to the cost function without regularization (which you implemented in  a previous lab), which is of the form:\n",
    "\n",
    "\n",
    "\\begin{equation} \n",
    "J(W) = \\frac{1}{2m} (XW - y)^T (XW - y)    \\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "The difference is the regularization term,  <span style=\"color:red\">\n",
    "    $\\frac{\\lambda}{2m}  \\sum_{j=1}^{n} w_j^2$ </span> and <span style=\"color:red\">\n",
    "    $\\frac{\\lambda}{2m} \\sum_{j=1}^{n} |w_j|$ \n",
    "    \n",
    "Including this term encourages gradient descent to minimize the size of the parameters.\n",
    "\n",
    " **Note, in this example, the parameter $w_0$ is not regularized. This is standard practice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd21f72",
   "metadata": {},
   "source": [
    "## Gradient descent with regularization\n",
    "The basic algorithm for running gradient descent does not change with regularization, it is:\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n} \\\\    \n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "Where each iteration performs simultaneous updates on $w_j$ for all $j$.\n",
    "\n",
    "What changes with regularization is computing the gradients. Let's try L2 regularization next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee53d1",
   "metadata": {},
   "source": [
    "## Gradient For L2 regularization \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(w)}{\\partial w_j} = 1/m\\sum_{i=1}^{m}(f_{\\mathbf{w}}(\\mathbf{x})^{(i)} - y^{(i)}).x_j^{(i)} + (\\lambda/m)x_j^{(i)}  \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "    \n",
    "The term which adds regularization is  the <span style=\"color:red\"> \n",
    "$\\frac{\\lambda}{m} w_j ^{(i)}$</span>, the parameter $w_0$ is not regularized\n",
    "\n",
    "**Vectorized form**\n",
    "\n",
    "\\begin{equation}\n",
    "W := W - \\alpha \\left( \\frac{1}{m} X^T (XW - y) + \\lambda W \\right)    \\tag{9}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62deb5eb",
   "metadata": {},
   "source": [
    "### Task 1: L2 Regularization Implementation (Ridge Regression)\n",
    "\n",
    "**Critical Note**: Do NOT regularize the bias term (w₀) - standard ML practice!\n",
    "\n",
    "**Objective**: Implement L2 regularization to combat overfitting through parameter shrinkage.\n",
    "\n",
    "- **Implementation**: Use vectorization for efficiency\n",
    "\n",
    "**Key Mathematical Changes:**- **Gradient**: Add λ * w regularization term to standard gradient\n",
    "- **Cost function**: Add λ/2 * ||w||₂² penalty term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fd198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_matrix_reg_l2(X, y, w, lambda_reg):\n",
    "    \"\"\"\n",
    "    🎯 Computes L2 regularized cost for linear regression (Ridge Regression)\n",
    "    \n",
    "    Mathematical Formula: \n",
    "    J(w) = (1/2m) * ||Xw - y||² + (λ/2) * ||w[1:]||²\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Feature matrix (including bias column)\n",
    "      y (ndarray (m,)): Target values\n",
    "      w (ndarray (n,)): Model parameters [bias, w1, w2, ..., wn]\n",
    "      lambda_reg (float): Regularization strength (λ)\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): Total cost = MSE + L2 penalty\n",
    "      \n",
    "    Implementation Notes:\n",
    "\n",
    "      - Don't regularize bias term: w[0] excluded from penalty\n",
    "\n",
    "      - Use w[1:] for regularization term   \n",
    "\n",
    "      - Vectorized: λ/2 * np.sum(w[1:]**2)    # cost = MSE_cost + (lambda_reg/2) * sum(w[1:]^2)\n",
    "\n",
    "    \"\"\"    # TODO: Implement L2 regularized cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980326ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the gradient with L2 regularization\n",
    "def compute_gradient_matrix_reg_l2(X, y, w, lambda_reg):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression with L2 regularization\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      lambda_reg (float): regularization parameter\n",
    "      \n",
    "    Returns\n",
    "      dj_dw (ndarray (n,1)): The gradient of the cost w.r.t. the parameters w.\n",
    "    \"\"\"\n",
    "    # use vectorization to implement the gradient for linear regression with L2 regularization, do not regularize the bias term\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f214e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform gradient descent with L2 regularization\n",
    "def gradient_descent_matrix_reg_l2(X, y, X_test, y_test, w_in, alpha, num_iters, lambda_reg):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w. Updates w by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (?,n+1))   : Training Data, m examples with n features\n",
    "      y (ndarray (?,))    : Training target values\n",
    "      X_test (ndarray (?,n+1))   : Test set, m examples with n features\n",
    "      y_test (ndarray (?,))    : target values in test set\n",
    "      w_in (ndarray (n+1,)) : initial model parameters  \n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      lambda_reg (float)  : regularization parameter\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n+1,)) : Updated values of parameters\n",
    "      J_history           : Cost of the model after each iteration\n",
    "      test_J_history      : Cost of the model on test set after each iteration\n",
    "      \"\"\"\n",
    "    \n",
    "    # implement the batch gradient descent algorithm with L2 regularization using the compute_gradient and compute_cost functions\n",
    "   \n",
    "\n",
    "        # print cost every 5000 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816463b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model parameters to 0\n",
    "w = np.zeros(X_train_scaled_1.shape[1])\n",
    "# run the gradient descent algorithm for 1000 iterations with a learning rate of 0.01 and lambda = 0.1\n",
    "alpha = 0.1\n",
    "num_iters = 60000\n",
    "lambda_l2 = 0.004\n",
    "GD_L2_w, J_history, test_J_history = gradient_descent_matrix_reg_l2()\n",
    "print(f\"w found by gradient descent with regularization: {w} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cost over the iterations\n",
    "plt.plot(J_history, label='Training')\n",
    "plt.plot(test_J_history, label='Test')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the above figures side by side for comparision, using subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "fig.suptitle('Train and Test Data')\n",
    "axs[0].plot(X_train[:, 0:1], X_train_scaled_1 @ GD_L2_w, 'r')\n",
    "axs[0].plot(X_train[:, 0:1], y_train, '.')\n",
    "axs[0].set_title('Train Data')\n",
    "axs[1].plot(X_test[:, 0:1], X_test_scaled_1 @ GD_L2_w, 'g')\n",
    "axs[1].plot(X_test[:, 0:1], y_test, '.')\n",
    "axs[1].set_title('Test Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38b685",
   "metadata": {},
   "source": [
    "### Task2: output the final RMSE for the training and test set below for performance comparision at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f35cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the final RMSE for the training and test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2020fc3",
   "metadata": {},
   "source": [
    "## Using L1 Regularization to fix the overfitting issue\n",
    "\n",
    "The L1 gradient formula can be found below.\n",
    "\n",
    "\\begin{equation}\n",
    "W := W - \\alpha \\left( \\frac{1}{m} X^T (XW - y) + \\lambda \\, \\text{sgn}(W) \\right)    \\tag{10}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### L1 Implementation Challenges: Understanding the Difficulties\n",
    "\n",
    "Using gradient descent for L1 regularization (Lasso) presents unique optimization challenges:\n",
    "\n",
    "#### Primary Challenge: Non-differentiability\n",
    "* **Problem**: The L1 penalty |w| is not differentiable at w = 0\n",
    "* **Impact**: Gradient becomes undefined at zero points, causing optimization instability\n",
    "* **Result**: Standard gradient descent may struggle near sparse solutions\n",
    "\n",
    "#### Professional Solutions:\n",
    "\n",
    "* **Coordinate Descent**: Update one parameter at a time (sklearn's approach)Let's implement L1 regularization and observe both its benefits and challenges!\n",
    "\n",
    "* **Subgradient Methods**: Use sign function approximation (our approach)\n",
    "\n",
    "* **Proximal Methods**: Advanced techniques for non-smooth optimization**Key Insight**: This is why professional libraries like sklearn use specialized algorithms rather than vanilla gradient descent for Lasso regression.\n",
    "\n",
    "* **Soft Thresholding**: Analytical solutions for certain cases\n",
    "\n",
    "We'll implement the subgradient method using `np.sign()` to approximate the L1 gradient. While this may show some convergence challenges, it demonstrates the core concepts and mathematical foundations.\n",
    "\n",
    "#### Our Educational Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167df22d",
   "metadata": {},
   "source": [
    "### Task 3: L1 Regularization Implementation (Lasso Regression)\n",
    "\n",
    "**Expected Outcome**: Some polynomial features should be eliminated (coefficients → 0)!\n",
    "\n",
    "**Objective**: Implement L1 regularization for automatic feature selection and overfitting prevention.\n",
    "\n",
    "- **Sparsity**: Expect some coefficients to become exactly zero\n",
    "\n",
    "**Key Implementation Details:**\n",
    "\n",
    "- **Sign function**: Use np.sign() for gradient computation\n",
    "\n",
    "- **Cost function**: Add λ * ||w||₁ penalty term\n",
    "- **Gradient**: Add λ * sign(w) to standard gradient  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the cost with L1 regularization\n",
    "def compute_cost_matrix_reg_l1(X, y, w, lambda_reg):\n",
    "    \"\"\"\n",
    "    Computes the cost for linear regression with L1 regularization\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      lambda_reg (float): regularization parameter\n",
    "      \n",
    "    Returns\n",
    "      cost: (scalar)\n",
    "    \"\"\"\n",
    "    # use vectorization to implement the cost for linear regression with L1 regularization, do not regularize the bias term\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the gradient with L1 regularization\n",
    "def compute_gradient_matrix_reg_l1(X, y, w, lambda_reg):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression with L1 regularization\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      lambda_reg (float): regularization parameter\n",
    "      \n",
    "    Returns\n",
    "      dj_dw (ndarray (n,1)): The gradient of the cost w.r.t. the parameters w.\n",
    "    \"\"\"\n",
    "    # use vectorization to implement the gradient for linear regression with L1 regularization, do not regularize the bias term\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform gradient descent with L1 regularization\n",
    "def gradient_descent_matrix_reg_l1(X, y, X_test, y_test, w_in, alpha, num_iters, lambda_reg):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w. Updates w by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (?,n+1))   : Training Data, m examples with n features\n",
    "      y (ndarray (?,))    : Training target values\n",
    "      X_test (ndarray (?,n+1))   : Test set, m examples with n features\n",
    "      y_test (ndarray (?,))    : target values in test set\n",
    "      w_in (ndarray (n+1,)) : initial model parameters  \n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      lambda_reg (float)  : regularization parameter\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n+1,)) : Updated values of parameters\n",
    "      J_history           : Cost of the model after each iteration\n",
    "      test_J_history      : Cost of the model on test set after each iteration\n",
    "      \"\"\"\n",
    "    \n",
    "    # implement the batch gradient descent algorithm using the compute_gradient and compute_cost functions\n",
    "   \n",
    "\n",
    "        # print cost every 5000 iterations\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf6acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model parameters to 0\n",
    "w = np.zeros(X_train_scaled_1.shape[1])\n",
    "# run the gradient descent algorithm for 1000 iterations with a learning rate of 0.01 and lambda = 0.1\n",
    "alpha = 0.1\n",
    "num_iters = 60000\n",
    "lambda_l1 = 0.004\n",
    "GD_L1_w, J_history, test_J_history = gradient_descent_matrix_reg_l1()\n",
    "print(f\"w found by gradient descent with L1 regularization: {w} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cost over the iterations\n",
    "plt.plot(J_history, label='Training')\n",
    "plt.plot(test_J_history, label='Test')\n",
    "\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f7d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the above figures side by side for comparision, using subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "fig.suptitle('Train and Test Data')\n",
    "axs[0].plot(X_train[:, 0:1], X_train_scaled_1 @ GD_L1_w, 'r')\n",
    "axs[0].plot(X_train[:, 0:1], y_train, '.')\n",
    "axs[0].set_title('Train Data')\n",
    "axs[1].plot(X_test[:, 0:1], X_test_scaled_1 @ GD_L1_w, 'g')\n",
    "axs[1].plot(X_test[:, 0:1], y_test, '.')\n",
    "axs[1].set_title('Test Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37f372",
   "metadata": {},
   "source": [
    "### Task4: output the final RMSE for the training and test set below for performance comparision at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the final RMSE for the training and test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2efc3",
   "metadata": {},
   "source": [
    "## Professional Implementation: Scikit-Learn's Optimized Algorithms\n",
    "\n",
    "\n",
    "**Why Compare with Sklearn?**\n",
    "\n",
    "- **Parameters**: Different hyperparameter conventions (alpha vs. lambda)\n",
    "\n",
    "- **Industry Standard**: Production-ready, optimized implementations\n",
    "- **Performance**: Optimized C/Fortran backends for speed\n",
    "\n",
    "- **Advanced Algorithms**: Coordinate descent for Lasso, efficient solvers for Ridge\n",
    "- **Stability**: Professional implementations handle edge cases better\n",
    "\n",
    "- **Performance Benchmark**: Compare your implementations with professional tools\n",
    "- **Convergence**: Sklearn may converge faster due to advanced optimization\n",
    "\n",
    "- **Real-World Relevance**: Understand when to build custom vs. use libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38592310",
   "metadata": {},
   "source": [
    "### Task 5: Professional Ridge Regression with Scikit-Learn\n",
    "\n",
    "**Comparison Focus**: How do sklearn's parameters compare to your custom implementation?\n",
    "\n",
    "**Implementation Requirements:**\n",
    "\n",
    "- Use `Ridge(alpha=0.004, max_iter=10000)` for fair comparison- Extract learned parameters using `.coef_` and `.intercept_`\n",
    "\n",
    "- Fit on scaled training data: `X_train_scaled`, `y_train`- Predict on both training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Ridge regression from sklearn to learn the model parameters\n",
    "# alpha=0.004, max_iter=10000 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531a17c",
   "metadata": {},
   "source": [
    "### Task6: Output the RMSE for the Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4800e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the final RMSE for the training and test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546a2a5",
   "metadata": {},
   "source": [
    "### Task 7: Professional Lasso Regression with Scikit-Learn\n",
    "\n",
    "**Sparsity Investigation**: Which polynomial terms does Lasso eliminate as irrelevant?\n",
    "\n",
    "**Implementation Requirements:**\n",
    "\n",
    "- Use `Lasso(alpha=0.004, max_iter=10000)` for consistency\n",
    "- Compare sparsity pattern with your custom L1 implementation\n",
    "\n",
    "- Apply to standardized data for proper regularization\n",
    "  \n",
    "**Key Analysis**: Count how many coefficients are exactly zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ecc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lasso regression from sklearn to learn the model parameters\n",
    "# alpha=0.004, max_iter=10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea76c77d",
   "metadata": {},
   "source": [
    "### Task 8: Lasso Performance Analysis\n",
    "\n",
    "**Evaluation Requirements:**\n",
    "- Calculate and output final RMSE for both training and test sets\n",
    "- **Sparsity Analysis**: Count and report how many coefficients are exactly zero\n",
    "- **Feature Selection**: Identify which polynomial terms were eliminated\n",
    "- **Compare**: How does Lasso sparsity compare to your custom L1 implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the final RMSE for the training and test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cafaa5",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis: Comparing All Approaches\n",
    "\n",
    "**Comparative Study Objectives:**\n",
    "- **Parameter Analysis**: How do learned weights differ across methods?\n",
    "- **Performance Evaluation**: Which approach generalizes best?\n",
    "- **Implementation Insights**: Custom vs. professional implementations  \n",
    "- **Method Selection**: When to use L1 vs. L2 vs. no regularization\n",
    "\n",
    "**Expected Discoveries:**\n",
    "- Regularized models should show better generalization (lower test RMSE)\n",
    "- L1 methods should produce sparse solutions with zero coefficients\n",
    "- L2 methods should shrink all parameters uniformly\n",
    "- Professional implementations may outperform custom gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f80302",
   "metadata": {},
   "source": [
    "### Task 9: Parameter Comparison Matrix\n",
    "\n",
    "**Create a comprehensive DataFrame comparing all learned parameters:**\n",
    "\n",
    "**Models to Include:**\n",
    "\n",
    "- Vanilla GD (baseline overfitted model)\n",
    "- Custom L2 Regularization (Ridge)\n",
    "- Custom L1 Regularization (Lasso) \n",
    "- Sklearn Ridge Regression\n",
    "- Sklearn Lasso Regression\n",
    "\n",
    "**DataFrame Structure:**\n",
    "\n",
    "- **Rows**: Each polynomial feature (x¹, x², ..., x¹⁵) + bias term\n",
    "- **Columns**: Each model's learned parameters\n",
    "- **Analysis**: Identify patterns, sparsity, parameter magnitude differences\n",
    "\n",
    "**Key Questions to Investigate:**\n",
    "\n",
    "1. Which coefficients does L1 drive to zero?\n",
    "2. How do L2 parameters compare in magnitude to unregularized?\n",
    "3. Are sklearn and custom implementations learning similar patterns?\n",
    "\n",
    "- **Rows**: Each polynomial feature (x¹, x², ..., x¹⁵) + bias term\n",
    "\n",
    "**Models to Include:**\n",
    "\n",
    "- Vanilla GD (baseline overfitted model)\n",
    "\n",
    "- Custom L2 Regularization (Ridge)- Sklearn Lasso Regression\n",
    "\n",
    "- Custom L1 Regularization (Lasso) - Sklearn Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b487a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting all the parameters from the above models in a dataframe, adding the bias term to the model parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b0b06",
   "metadata": {},
   "source": [
    "### Task 10: Performance Leaderboard Analysis\n",
    "\n",
    "**Expected Insights**: Regularized models should show better generalization!\n",
    "\n",
    "**Create a comprehensive performance comparison:**\n",
    "\n",
    "* How do custom implementations compare to sklearn?\n",
    "* When would you choose L1 vs L2 regularization?\n",
    "\n",
    "**Metrics to Compare:**\n",
    "\n",
    "**Metrics to Compare:**\n",
    "\n",
    "- Training RMSE (overfitting indicator)\n",
    "- Test RMSE (generalization measure) \n",
    "- Generalization Gap (Test RMSE - Train RMSE)\n",
    "- Number of Zero Coefficients (sparsity measure)\n",
    "\n",
    "**Analysis Questions:**\n",
    "\n",
    "1. Which method has the best test performance?\n",
    "2. Which method shows least overfitting (smallest gap)?\n",
    "3. How do custom implementations compare to sklearn?\n",
    "4. When would you choose L1 vs L2 regularization?\n",
    "\n",
    "**Expected Insights**: Regularized models should show better generalization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0720aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all the RMSE values in a dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4aac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4df9f5a4",
   "metadata": {},
   "source": [
    "## Congratulations! You've Mastered Regularized Machine Learning!\n",
    "\n",
    "\n",
    "### Key Achievements Unlocked:\n",
    "\n",
    "#### Technical Mastery\n",
    "\n",
    "* **Regularized Cost Functions**: Successfully extended MSE with L1/L2 penalty terms\n",
    "* **Advanced Gradient Descent**: Implemented vectorized algorithms with regularization\n",
    "* **Professional Tools**: Leveraged scikit-learn's optimized Lasso and Ridge implementations\n",
    "* **Comparative Analysis**: Systematically evaluated custom vs. professional approaches\n",
    "\n",
    "#### Conceptual Understanding \n",
    "\n",
    "* **Overfitting Recognition**: Identified and diagnosed overfitting through learning curves\n",
    "* **Bias-Variance Tradeoff**: Observed how regularization balances model complexity\n",
    "* **L1 vs L2 Intuition**: Understood sparsity (L1) vs. shrinkage (L2) effects\n",
    "* **Hyperparameter Impact**: Explored how λ controls regularization strength\n",
    "\n",
    "#### Professional Insights\n",
    "\n",
    "* **When to Regularize**: Small datasets + high dimensions = regularization essential\n",
    "* **Method Selection**: L1 for feature selection, L2 for smooth parameter shrinkage\n",
    "* **Implementation Choices**: Custom for learning, sklearn for production\n",
    "* **Performance Evaluation**: Always compare training vs. test performance\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "You're now equipped to tackle overfitting in:\n",
    "\n",
    "- **Medical Diagnosis**: High-dimensional genomic data\n",
    "- **Financial Modeling**: Feature-rich economic indicators \n",
    "- **Image Recognition**: Pixel-level feature engineering\n",
    "- **Natural Language Processing**: Large vocabulary models\n",
    "\n",
    "**Next Steps**: Apply these techniques to your own datasets and explore advanced regularization methods like Elastic Net!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7b4d0",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "Deeplearning.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ff7fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
