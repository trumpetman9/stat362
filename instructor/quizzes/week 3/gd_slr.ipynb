{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b7586d88-1b36-42f6-9cf4-2d01cf6c5204",
   "metadata": {
    "id": "b7586d88-1b36-42f6-9cf4-2d01cf6c5204",
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Quiz 1: Gradient Descent with Simple Linear Regression Using For Loop\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    number-sections: true\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a8da5-27b2-4eef-84a0-70a354fcb50b",
   "metadata": {
    "id": "038a8da5-27b2-4eef-84a0-70a354fcb50b"
   },
   "source": [
    "## Tools and Libraries\n",
    "\n",
    "In this quiz, we will make use of:\n",
    "\n",
    "- **NumPy** - A fundamental library for scientific computing in Python, providing support for arrays, mathematical functions, and linear algebra operations\n",
    "- **Matplotlib** - A comprehensive library for creating static, animated, and interactive visualizations in Python\n",
    "- **Built-in Python functions** - Including `copy` for creating deep copies of objects and `math` for mathematical operations\n",
    "- **Jupyter notebook magic commands** - Such as `%matplotlib inline` for displaying plots inline and `%autoreload` for automatically reloading modules\n",
    "\n",
    "## Key Python Concepts Used\n",
    "- For loops for iterative computations\n",
    "- NumPy arrays for efficient numerical operations\n",
    "- Function definitions and parameter passing\n",
    "- Gradient descent algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cc0416-20b6-4d53-88ce-b45b6f0a4acb",
   "metadata": {
    "id": "a6cc0416-20b6-4d53-88ce-b45b6f0a4acb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153d9384-e918-4346-b704-fe59b219b57c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "153d9384-e918-4346-b704-fe59b219b57c",
    "outputId": "dce00b22-ea03-4f04-c4c1-0bb6a3208d29"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ed48f",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "### Real Estate Price Prediction Challenge\n",
    "\n",
    "Imagine you're a real estate analyst tasked with building a model to predict house prices based on square footage. You have access to a small but valuable dataset from recent sales in your area.\n",
    "\n",
    "**Our Dataset:**\n",
    "We have two recent home sales that will serve as our training data:\n",
    "\n",
    "| House | Size (1000 sqft) | Price (1000s of dollars) | Details |\n",
    "|-------|------------------|--------------------------|---------|\n",
    "| A     | 1.0              | 300                      | 1000 sqft ‚Üí $300,000 |\n",
    "| B     | 2.0              | 500                      | 2000 sqft ‚Üí $500,000 |\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this quiz, you will:\n",
    "\n",
    "1. **Implement from scratch** the three core components of gradient descent:\n",
    "   - Cost function computation\n",
    "   - Gradient calculation  \n",
    "   - Parameter optimization loop\n",
    "2. **Apply mathematical concepts** using Python for loops (no vectorization shortcuts!)\n",
    "3. **Visualize the learning process** through cost function plots\n",
    "4. **Make predictions** using your trained model\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "Your mission is to find the optimal parameters $(w, b)$ for the linear model:\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x$ = house size in thousands of square feet\n",
    "- $f_{w,b}(x)$ = predicted price in thousands of dollars\n",
    "- $w$ = slope (price increase per 1000 sqft)\n",
    "- $b$ = y-intercept (base price)\n",
    "\n",
    "**Question to ponder:** Looking at our data, can you estimate what the slope $w$ might be? What does this represent in real-world terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75428a4e-f708-4a61-9a56-288af4599b4a",
   "metadata": {
    "id": "75428a4e-f708-4a61-9a56-288af4599b4a"
   },
   "outputs": [],
   "source": [
    "# Create our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba9a7b-7ee9-49fd-953a-e0fd1f7b2e0c",
   "metadata": {
    "id": "e3ba9a7b-7ee9-49fd-953a-e0fd1f7b2e0c"
   },
   "source": [
    "## Linear Regression Fundamentals\n",
    "\n",
    "In this quiz, you will fit the linear regression parameters $(w,b)$ to your dataset using gradient descent from scratch.\n",
    "\n",
    "### The Linear Model\n",
    "\n",
    "The model function for linear regression maps from input `x` (house size) to output `y` (house price):\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w$ is the **weight** (or slope) - how much the price changes per unit of size\n",
    "- $b$ is the **bias** (or y-intercept) - the base price when size = 0\n",
    "- $x$ is the input feature (house size in 1000s of sqft)\n",
    "- $f_{w,b}(x)$ is the predicted output (house price in 1000s of dollars)\n",
    "\n",
    "### Finding the Best Parameters\n",
    "\n",
    "To train a linear regression model, you need to find the optimal $(w,b)$ parameters:\n",
    "\n",
    "1. **Cost Function Evaluation**: Compare different parameter choices using a cost function $J(w,b)$\n",
    "   - $J(w,b)$ measures how well your model fits the data\n",
    "   - Lower cost = better fit\n",
    "   \n",
    "2. **Optimization Goal**: Find $(w,b)$ that minimizes $J(w,b)$\n",
    "   - The \"best\" parameters are those with the smallest cost\n",
    "   - This gives you the line that best fits your training data\n",
    "\n",
    "3. **Gradient Descent**: Use this iterative algorithm to find optimal parameters\n",
    "   - Start with initial guesses for $w$ and $b$\n",
    "   - Repeatedly adjust parameters in the direction that reduces cost\n",
    "   - Each step moves closer to the optimal values\n",
    "\n",
    "### The Power of Your Model\n",
    "\n",
    "Once trained, your linear regression model becomes a **prediction machine**:\n",
    "\n",
    "- **Input**: Square footage of any house\n",
    "- **Output**: Estimated selling price\n",
    "- **Application**: Help real estate agents, buyers, and sellers make informed decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817cde90-255a-4bf0-945a-a904c5008e84",
   "metadata": {
    "id": "817cde90-255a-4bf0-945a-a904c5008e84"
   },
   "source": [
    "## Implement Gradient Descent From Scratch\n",
    "\n",
    "Now comes the exciting part! You will implement the gradient descent algorithm step by step using **for loops only** - no vectorized operations allowed. This approach will help you understand exactly what's happening at each step.\n",
    "\n",
    "### The Three Essential Functions\n",
    "\n",
    "You'll build three interconnected functions that work together:\n",
    "\n",
    "1. **`compute_cost`** - Measures how well your current parameters fit the data\n",
    "2. **`compute_gradient`** - Calculates which direction to adjust your parameters  \n",
    "3. **`gradient_descent`** - Orchestrates the iterative optimization process\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "   ```\n",
    "   Input: Training Data (x, y)\n",
    "      ‚Üì\n",
    "   compute_cost(w, b) ‚Üí Current fit quality\n",
    "      ‚Üì\n",
    "   compute_gradient(w, b) ‚Üí Direction to improve\n",
    "      ‚Üì  \n",
    "   gradient_descent() ‚Üí New improved (w, b)\n",
    "      ‚Üì\n",
    "   Repeat until convergence!\n",
    "   ```\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "**Why for loops?** While NumPy vectorization is faster, using explicit loops helps you:\n",
    "\n",
    "- Understand each calculation step-by-step\n",
    "- Build intuition for how gradient descent actually works\n",
    "- Appreciate vectorization when you use it later!\n",
    "\n",
    "### Coding Conventions\n",
    "\n",
    "To keep our code readable and mathematically accurate:\n",
    "\n",
    "- **Partial derivatives**: Variables representing $\\frac{\\partial J(w,b)}{\\partial b}$ will be named `dj_db`\n",
    "- **Naming pattern**: `dj_d[parameter]` where `[parameter]` is the variable we're taking the derivative with respect to\n",
    "- **Abbreviation**: \"w.r.t\" = \"With Respect To\" (common mathematical shorthand)\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- $\\frac{\\partial J(w,b)}{\\partial w}$ ‚Üí `dj_dw` (derivative of J with respect to w)  \n",
    "- $\\frac{\\partial J(w,b)}{\\partial b}$ ‚Üí `dj_db` (derivative of J with respect to b)\n",
    "\n",
    "### Ready to Code?\n",
    "\n",
    "Let's implement each function one by one, building your gradient descent algorithm from the ground up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7777fd-33b5-43ef-b973-3bcd3f89141b",
   "metadata": {
    "id": "3c7777fd-33b5-43ef-b973-3bcd3f89141b"
   },
   "source": [
    "### Function 1: Compute_Cost\n",
    "\n",
    "The cost function is your **quality meter** - it tells you how well your current parameters $(w,b)$ fit the training data. Lower cost means better fit!\n",
    "\n",
    "#### Understanding the Cost Function\n",
    "\n",
    "The Mean Squared Error (MSE) cost function measures the average squared difference between predictions and actual values:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "**Breaking it down:**\n",
    "\n",
    "- $f_{w,b}(x^{(i)}) = wx^{(i)} + b$ ‚Üí Your model's prediction for house $i$\n",
    "- $y^{(i)}$ ‚Üí Actual selling price for house $i$  \n",
    "- $(f_{w,b}(x^{(i)}) - y^{(i)})^2$ ‚Üí Squared error for house $i$\n",
    "- $\\frac{1}{2m}$ ‚Üí Average over all $m$ examples (the $\\frac{1}{2}$ simplifies calculus later!)\n",
    "\n",
    "#### Task 1: compute the cost\n",
    "\n",
    "Complete the `compute_cost` function using for loops to:\n",
    "\n",
    "**Step 1:** For each training example $i$, compute:\n",
    "\n",
    "- **Prediction:** $f_{wb}(x^{(i)}) = wx^{(i)} + b$\n",
    "- **Individual cost:** $cost^{(i)} = (f_{wb}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "**Step 2:** Sum all individual costs and return the total:\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} cost^{(i)}$$\n",
    "\n",
    "**Key insight:** You're measuring how \"wrong\" your predictions are on average. The squaring ensures:\n",
    "\n",
    "- All errors are positive (no cancellation between over/under predictions)\n",
    "- Larger errors are penalized more heavily than small errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed6956-9545-48f9-8d7a-644c1cc1d35e",
   "metadata": {
    "id": "9aed6956-9545-48f9-8d7a-644c1cc1d35e"
   },
   "outputs": [],
   "source": [
    "# compute_cost\n",
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression using Mean Squared Error.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) - Input features (house sizes in 1000s sqft)\n",
    "        y (ndarray): Shape (m,) - Target values (house prices in 1000s dollars)\n",
    "        w (scalar): Weight parameter (slope of the line)\n",
    "        b (scalar): Bias parameter (y-intercept of the line)\n",
    "\n",
    "    Returns:\n",
    "        total_cost (float): The cost J(w,b) representing how well the parameters\n",
    "                           fit the training data. Lower cost = better fit.\n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "\n",
    "    # Initialize total cost\n",
    "    total_cost = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Variable to accumulate the sum of squared errors\n",
    "    cost_sum = 0\n",
    "    \n",
    "    # Loop through each training example\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Step 1: Calculate prediction using linear model\n",
    "        f_wb = # YOUR CODE\n",
    "        \n",
    "        # Step 2: Calculate squared error for this example\n",
    "        error = # YOUR CODE\n",
    "        cost = # YOUR CODE\n",
    "        \n",
    "        # Step 3: Add this example's cost to running sum\n",
    "        cost_sum = # YOUR CODE\n",
    "    \n",
    "    # Step 4: Calculate final cost using MSE \n",
    "    total_cost = # YOUR CODE\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad95f76-8295-423b-9337-594bc7fc8c73",
   "metadata": {
    "id": "dad95f76-8295-423b-9337-594bc7fc8c73"
   },
   "source": [
    "#### Test Your Implementation\n",
    "\n",
    "Great job! Now let's verify that your `compute_cost` function works correctly. \n",
    "\n",
    "Run the test code below to check your implementation. The test uses initial parameters `w=2` and `b=1` to see how well they fit our training data.\n",
    "\n",
    "**What to expect:**\n",
    "\n",
    "- The function should return a `float` type\n",
    "- The cost value tells you how well these parameters fit the data\n",
    "- Higher cost = worse fit, Lower cost = better fit\n",
    "\n",
    "**For the Canvas quiz:** Record the exact cost value that your function returns - you'll need this number to complete the quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost with some initial values for paramaters w, b\n",
    "initial_w = 2\n",
    "initial_b = 1\n",
    "\n",
    "cost = compute_cost(x_train, y_train, initial_w, initial_b)\n",
    "print(type(cost))\n",
    "print(f'Cost at initial w: {cost:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f01902b",
   "metadata": {},
   "source": [
    "### Function 2: Compute_Gradient\n",
    "\n",
    "The gradient tells you **which direction to move** your parameters to reduce the cost. Think of it as a compass pointing toward better parameter values!\n",
    "\n",
    "#### Understanding Gradients\n",
    "\n",
    "Gradients are partial derivatives that measure how the cost function changes when you slightly adjust each parameter:\n",
    "\n",
    "- $\\frac{\\partial J(w,b)}{\\partial w}$ ‚Üí How much does cost change if we increase $w$ slightly?\n",
    "- $\\frac{\\partial J(w,b)}{\\partial b}$ ‚Üí How much does cost change if we increase $b$ slightly?\n",
    "\n",
    "**The math behind it:** For our cost function $J(w,b) = \\frac{1}{2m} \\sum (f_{w,b}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "The partial derivatives work out to:\n",
    "$$\\frac{\\partial J(w,b)}{\\partial b}^{(i)} = (f_{w,b}(x^{(i)}) - y^{(i)})$$\n",
    "$$\\frac{\\partial J(w,b)}{\\partial w}^{(i)} = (f_{w,b}(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}$$\n",
    "\n",
    "#### Task 2: compute the gradient\n",
    "\n",
    "Complete the `compute_gradient` function using for loops to:\n",
    "\n",
    "**Step 1:** For each training example $i$, compute:\n",
    "\n",
    "- **Prediction:** $f_{wb}(x^{(i)}) = wx^{(i)} + b$\n",
    "- **Error:** $error^{(i)} = f_{wb}(x^{(i)}) - y^{(i)}$ \n",
    "- **Gradient contributions:**\n",
    "  - For $b$: $\\frac{\\partial J}{\\partial b}^{(i)} = error^{(i)}$\n",
    "  - For $w$: $\\frac{\\partial J}{\\partial w}^{(i)} = error^{(i)} \\cdot x^{(i)}$\n",
    "\n",
    "**Step 2:** Average all gradient contributions:\n",
    "$$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum\\limits_{i=0}^{m-1} \\frac{\\partial J}{\\partial b}^{(i)}$$\n",
    "$$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum\\limits_{i=0}^{m-1} \\frac{\\partial J}{\\partial w}^{(i)}$$\n",
    "\n",
    "**Key insight:** \n",
    "\n",
    "- **Positive gradient** ‚Üí Increase parameter to reduce cost\n",
    "- **Negative gradient** ‚Üí Decrease parameter to reduce cost  \n",
    "- **Larger magnitude** ‚Üí Steeper slope, bigger adjustment needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddb976-e672-4c7c-bf17-7f5f3b9b459f",
   "metadata": {
    "id": "64ddb976-e672-4c7c-bf17-7f5f3b9b459f"
   },
   "outputs": [],
   "source": [
    "# compute_gradient\n",
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) Input to the model (house sizes)\n",
    "      y (ndarray): Shape (m,) Label (house prices)\n",
    "      w, b (scalar): Parameters of the model\n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameter w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
    "     \"\"\"\n",
    "\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    # Initialize gradient accumulators\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Loop through each training example\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Step 1: Calculate prediction for example i\n",
    "        f_wb = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 2: Calculate the error (prediction - actual)\n",
    "        error = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 3: Calculate gradient contributions for this example\n",
    "        dj_db_i = # YOUR CODE HERE\n",
    "        \n",
    "        dj_dw_i = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 4: Accumulate the gradients\n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i\n",
    "    \n",
    "    # Step 5: Average the gradients over all examples\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e24e3-1fce-469f-97ef-9a63a9ca9254",
   "metadata": {
    "id": "d58e24e3-1fce-469f-97ef-9a63a9ca9254"
   },
   "source": [
    "#### Test Your Gradient Function\n",
    "\n",
    "Excellent work! Now let's test your `compute_gradient` function to see if it correctly calculates the gradients.\n",
    "\n",
    "Run the test below to verify your implementation. We'll start with parameters `w=0` and `b=0` to see what gradients your function computes.\n",
    "\n",
    "**What to expect:**\n",
    "\n",
    "- The function should return two values: `dj_dw` and `dj_db`\n",
    "- These represent the direction and magnitude to adjust each parameter\n",
    "- The values tell you how to improve your model's fit to the data\n",
    "\n",
    "**For the Canvas quiz:** Record the exact gradient values that your function returns - you'll need these numbers for the quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8cd99-37b3-46e8-bad9-c8194fadf8a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33a8cd99-37b3-46e8-bad9-c8194fadf8a9",
    "outputId": "41ef6c10-376f-4d7f-a8a6-b7eac22013a2"
   },
   "outputs": [],
   "source": [
    "# Compute and display gradient with w and b initialized to zeroes\n",
    "initial_w = 0\n",
    "initial_b = 0\n",
    "\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)\n",
    "print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d647d55",
   "metadata": {},
   "source": [
    "### Function 3: Do Gradient Decent\n",
    "\n",
    "After you implemented `compute_gradient` which calculates $\\frac{\\partial J(w)}{\\partial w}$, $\\frac{\\partial J(w)}{\\partial b}$ , you will next implement the gradient descent for parameters $w, b$ for linear regression.\n",
    "\n",
    "As described in the lecture, the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & \\phantom {0000} b := b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline       \\; & \\phantom {0000} w := w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w}   \\; &\n",
    "\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $w, b$ are both updated simultaniously and where  \n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)} \n",
    "$$\n",
    "\n",
    "* $m$ is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{w,b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$, is the target value\n",
    "a.lgorithm.\n",
    "\n",
    "#### Task 3: do gradient descent\n",
    "\n",
    "Please complete the `gradient_descent` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8d754-3045-4492-908b-9f74df6c70fd",
   "metadata": {
    "id": "ddf8d754-3045-4492-908b-9f74df6c70fd"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to optimize parameters w and b\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) - Input features (house sizes)\n",
    "      y (ndarray): Shape (m,) - Target values (house prices)  \n",
    "      w_in, b_in (scalar): Initial parameter values\n",
    "      cost_function: Function to compute cost J(w,b)\n",
    "      gradient_function: Function to compute gradients dJ/dw, dJ/db\n",
    "      alpha (float): Learning rate - how big steps to take\n",
    "      num_iters (int): Number of iterations to run\n",
    "    Returns:\n",
    "      w, b (scalar): Optimized parameter values\n",
    "      J_history (list): Cost at each iteration (for plotting)\n",
    "      w_history (list): Parameter values at each iteration (for plotting)\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of training examples\n",
    "    m = len(x)\n",
    "    \n",
    "    # Arrays to store history for plotting learning curves\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    # Initialize parameters (make copies to avoid modifying inputs)\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Main gradient descent loop\n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        # Step 1: Compute gradients using your gradient function\n",
    "        dj_dw, dj_db = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 2: Update parameters using gradient descent rule\n",
    "        w = # YOUR CODE HERE\n",
    "        b = # YOUR CODE HERE\n",
    "        \n",
    "        # Step 3: Compute and save cost for this iteration (for plotting)\n",
    "        if i < 100000:  # Prevent memory issues for very long runs\n",
    "            cost = # YOUR CODE HERE \n",
    "            J_history.append(cost)\n",
    "            w_history.append([w, b])\n",
    "        \n",
    "        # Print progress every 10% of iterations\n",
    "        if i % math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return w, b, J_history, w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48d490-3ec7-4ade-8c81-5fc561aadc0f",
   "metadata": {
    "id": "ee48d490-3ec7-4ade-8c81-5fc561aadc0f"
   },
   "source": [
    "\n",
    "####  Find Your Optimal Parameters via your gradient descent\n",
    "\n",
    "Now let's run your gradient descent algorithm to discover the optimal values of $w$ and $b$ for our real estate dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0951a1-073d-4895-9bb1-3b7668e2cde3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e0951a1-073d-4895-9bb1-3b7668e2cde3",
    "outputId": "22154047-5730-4eac-a25a-d887ab7ed8a4"
   },
   "outputs": [],
   "source": [
    "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
    "initial_w = 0.\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w, b, J_hist, p_hist = gradient_descent(x_train ,y_train, initial_w, initial_b,\n",
    "                     compute_cost, compute_gradient, alpha, iterations)\n",
    "print(\"w,b found by gradient descent:\", w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b7112-7a06-481e-8daf-f0852fd885ec",
   "metadata": {
    "id": "867b7112-7a06-481e-8daf-f0852fd885ec"
   },
   "source": [
    "## üìà Visualizing the Learning Process: Cost Function Evolution\n",
    "\n",
    "### Understanding Your Algorithm's Journey\n",
    "\n",
    "Now comes one of the most exciting parts - watching your gradient descent algorithm learn in real time! The learning curve visualization will show you exactly how your algorithm improved with each iteration.\n",
    "\n",
    "### What You'll Observe in the Plots\n",
    "\n",
    "**Two-Panel Visualization:**\n",
    "\n",
    "1. **Left Panel (Early Learning):** Shows the first 100 iterations\n",
    "   \n",
    "   - **Expect:** Sharp, rapid decrease in cost\n",
    "   - **Why:** Initial parameter values are far from optimal, so big improvements happen quickly\n",
    "   - **Learning:** This demonstrates the power of gradient descent's initial convergence\n",
    "\n",
    "2. **Right Panel (Fine-Tuning):** Shows iterations 1000+ to the end\n",
    "   \n",
    "   - **Expect:** Gradual, steady decrease approaching a minimum\n",
    "   - **Why:** Algorithm is fine-tuning parameters near the optimal solution\n",
    "   - **Learning:** Shows how gradient descent achieves precision through patience\n",
    "\n",
    "### Key Learning Insights\n",
    "\n",
    "**Cost Decrease Patterns:**\n",
    "\n",
    "- **Steep drop** ‚Üí Your algorithm is making big improvements (far from optimum)\n",
    "- **Gradual decline** ‚Üí Fine-tuning phase (approaching optimum)  \n",
    "- **Plateau** ‚Üí Convergence achieved (found the minimum!)\n",
    "\n",
    "**What This Tells You:**\n",
    "\n",
    "- **Algorithm Health:** Consistently decreasing cost = healthy learning\n",
    "- **Learning Rate Quality:** Smooth curves = good learning rate choice\n",
    "- **Convergence Status:** Flattening curve = approaching optimal solution\n",
    "\n",
    "**For the Canvas Quiz:** Pay attention to the final cost value and how many iterations it took to converge - these observations may be part of your quiz questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41184459-21ed-4bbb-a371-24409929c056",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "41184459-21ed-4bbb-a371-24409929c056",
    "outputId": "993fe0ea-76e5-4126-851b-0571db16486c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot cost versus iteration\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a840ee-f302-4c1d-8f31-b79518f24ee7",
   "metadata": {
    "id": "90a840ee-f302-4c1d-8f31-b79518f24ee7"
   },
   "source": [
    "##  Making Predictions with Your Trained Model\n",
    "\n",
    "### Task 4: Real Estate Price Prediction Challenge\n",
    "\n",
    "**The Moment of Truth!** \n",
    "Now that you have trained your model and found optimal parameters $w$ and $b$, it's time to put your **prediction machine** to work! This is where all your hard work pays off.\n",
    "\n",
    "### Understanding Your Prediction Tool\n",
    "\n",
    "**The Prediction Formula:**\n",
    "Your linear regression model makes predictions using the simple but powerful equation:\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b$$\n",
    "\n",
    "**Breaking Down Each Component:**\n",
    "\n",
    "- **$w$ (slope)** = Price increase per 1000 sqft ‚Üí *\"How much more does each additional 1000 sqft cost?\"*\n",
    "- **$b$ (y-intercept)** = Base price when size = 0 ‚Üí *\"What's the starting price before considering square footage?\"*\n",
    "- **$x^{(i)}$ (input)** = House size in thousands of sqft ‚Üí *\"The feature we're using to predict\"*\n",
    "- **$f_{w,b}(x^{(i)})$ (output)** = Predicted price in thousands of dollars ‚Üí *\"Our model's best guess\"*\n",
    "\n",
    "###  Your Real Estate Consultation Challenge\n",
    "\n",
    "**The Scenario:**\n",
    "\n",
    "A potential buyer walks into your real estate office and asks:\n",
    "\n",
    "> *\"I'm interested in buying a house with exactly **1200 square feet**. Based on recent market data, what should I expect to pay?\"*\n",
    "\n",
    "**Your Mission:**\n",
    "\n",
    "1. **Use your trained parameters** $(w, b)$ found by gradient descent\n",
    "2. **Convert units properly** (1200 sqft ‚Üí thousands of sqft)\n",
    "3. **Calculate the prediction** using your linear model\n",
    "4. **Provide a professional estimate** to your client\n",
    "\n",
    "\n",
    "**For the Canvas Quiz:** üìù Record your predicted price carefully - you'll need this exact number for the quiz questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "SF2ScjY1RYJ8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF2ScjY1RYJ8",
    "outputId": "6c2211ce-6bba-4eae-c560-ad7e3203d795"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# House size: 1200 sqft\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccd1ed",
   "metadata": {},
   "source": [
    "##  Bonus Exploration: Learning Rate Sensitivity Analysis\n",
    "\n",
    "**Experiment Goal:** Run the following code cells and investigate how different learning rates affect gradient descent convergence and performance\n",
    "\n",
    "###  The Learning Rate Experiment\n",
    "\n",
    "Learning rate (Œ±) is one of the most critical hyperparameters in machine learning. This experiment will demonstrate how this single parameter can dramatically affect your algorithm's behavior, convergence speed, and final performance.\n",
    "\n",
    "###  Experimental Design\n",
    "\n",
    "**Test Parameters:**\n",
    "- Learning rates: 0.001, 0.01, 0.1, 0.5\n",
    "- Iterations: 1000 (sufficient to observe different behaviors)\n",
    "- Initial conditions: w=0, b=0 (same for all tests)\n",
    "- Metric: Cost function evolution over time\n",
    "\n",
    "**Hypothesis:** Different learning rates will show distinct convergence patterns - some too slow, some too fast, and hopefully one \"just right\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb62d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using different learning rates to compare convergence behavior\n",
    "# This helps understand how learning rate affects training dynamics\n",
    "\n",
    "# Define different learning rates to test\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "colors = ['blue', 'green', 'red', 'orange']\n",
    "iterations = 1000\n",
    "\n",
    "# Store results for each learning rate\n",
    "results = {}\n",
    "\n",
    "print(\"Testing different learning rates:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    print(f\"\\nTesting learning rate: {alpha}\")\n",
    "    \n",
    "    # Reset initial parameters for fair comparison\n",
    "    initial_w = 0.0\n",
    "    initial_b = 0.0\n",
    "    \n",
    "    # Run gradient descent with current learning rate\n",
    "    w_final, b_final, J_history, p_history = gradient_descent(\n",
    "        x_train, y_train, initial_w, initial_b, \n",
    "        compute_cost, compute_gradient, alpha, iterations\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[alpha] = {\n",
    "        'w': w_final,\n",
    "        'b': b_final,\n",
    "        'cost_history': J_history,\n",
    "        'final_cost': J_history[-1] if J_history else float('inf')\n",
    "    }\n",
    "    \n",
    "    print(f\"Final parameters: w={w_final:.6f}, b={b_final:.6f}\")\n",
    "    print(f\"Final cost: {J_history[-1]:.8f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Learning Rate Comparison Summary:\")\n",
    "for alpha in learning_rates:\n",
    "    result = results[alpha]\n",
    "    print(f\"Œ± = {alpha:5.3f}: Final Cost = {result['final_cost']:.8f}, w = {result['w']:.6f}, b = {result['b']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3183e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of learning curves\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: All learning curves together (full range)\n",
    "ax1.set_title('Learning Curves: All Learning Rates (Full Range)', fontsize=14, fontweight='bold')\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    cost_history = results[alpha]['cost_history']\n",
    "    ax1.plot(cost_history, color=colors[i], label=f'Œ± = {alpha}', linewidth=2)\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Cost J(w,b)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Early iterations (first 100) - shows initial convergence behavior\n",
    "ax2.set_title('Early Learning Phase (First 100 Iterations)', fontsize=14, fontweight='bold')\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    cost_history = results[alpha]['cost_history']\n",
    "    early_history = cost_history[:min(100, len(cost_history))]\n",
    "    ax2.plot(early_history, color=colors[i], label=f'Œ± = {alpha}', linewidth=2)\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Cost J(w,b)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Log scale view - better for comparing convergence rates\n",
    "ax3.set_title('Learning Curves (Log Scale) - Convergence Analysis', fontsize=14, fontweight='bold')\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    cost_history = results[alpha]['cost_history']\n",
    "    # Add small epsilon to avoid log(0) issues\n",
    "    log_costs = [max(cost, 1e-10) for cost in cost_history]\n",
    "    ax3.semilogy(log_costs, color=colors[i], label=f'Œ± = {alpha}', linewidth=2)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Cost J(w,b) (Log Scale)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Final cost comparison (bar chart)\n",
    "ax4.set_title('Final Cost Comparison After 1000 Iterations', fontsize=14, fontweight='bold')\n",
    "final_costs = [results[alpha]['final_cost'] for alpha in learning_rates]\n",
    "bars = ax4.bar(range(len(learning_rates)), final_costs, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Learning Rate')\n",
    "ax4.set_ylabel('Final Cost')\n",
    "ax4.set_xticks(range(len(learning_rates)))\n",
    "ax4.set_xticklabels([f'Œ± = {alpha}' for alpha in learning_rates])\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, cost) in enumerate(zip(bars, final_costs)):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(final_costs)*0.01, \n",
    "             f'{cost:.6f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis and insights\n",
    "print(\"\\nüîç LEARNING RATE ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find best performing learning rate\n",
    "best_alpha = min(learning_rates, key=lambda x: results[x]['final_cost'])\n",
    "worst_alpha = max(learning_rates, key=lambda x: results[x]['final_cost'])\n",
    "\n",
    "print(f\"üèÜ BEST Learning Rate: Œ± = {best_alpha}\")\n",
    "print(f\"   Final Cost: {results[best_alpha]['final_cost']:.8f}\")\n",
    "print(f\"   Parameters: w = {results[best_alpha]['w']:.6f}, b = {results[best_alpha]['b']:.6f}\")\n",
    "\n",
    "print(f\"\\n‚ùå WORST Learning Rate: Œ± = {worst_alpha}\")\n",
    "print(f\"   Final Cost: {results[worst_alpha]['final_cost']:.8f}\")\n",
    "print(f\"   Parameters: w = {results[worst_alpha]['w']:.6f}, b = {results[worst_alpha]['b']:.6f}\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"   ‚Ä¢ Lower learning rates ‚Üí More stable but slower convergence\")\n",
    "print(\"   ‚Ä¢ Higher learning rates ‚Üí Faster initial progress but may overshoot\")\n",
    "print(\"   ‚Ä¢ Optimal learning rate balances speed and stability\")\n",
    "print(\"   ‚Ä¢ Learning rate choice significantly impacts training efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c009e",
   "metadata": {},
   "source": [
    "###  Your Insights and Reflection\n",
    "\n",
    "**Analyze the results above and answer these questions:**\n",
    "\n",
    "####  Observation Questions:\n",
    "\n",
    "1. **Which learning rate achieved the lowest final cost? Why do you think this happened?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "   \n",
    "2. **Compare the convergence speed between Œ±=0.001 and Œ±=0.1. What trade-offs do you observe?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "   \n",
    "3. **Looking at the early convergence plot (first 100 iterations), which learning rate made the most dramatic initial progress? Was this rate also the best overall performer?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "\n",
    "####  Critical Thinking Questions:\n",
    "\n",
    "4. **If you had to choose ONE learning rate for a production machine learning system, which would you choose and why?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "   \n",
    "\n",
    "5. **What would you expect to happen if we tested an even larger learning rate like Œ±=1.0? Explain your reasoning.**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "\n",
    "6. **How might the optimal learning rate change if we had:**\n",
    "   - A much larger dataset (1000s of examples)?\n",
    "   - A more complex cost function landscape?\n",
    "   \n",
    "   *Your answers:*\n",
    "   \n",
    "\n",
    "####  Real-World Application:\n",
    "\n",
    "7. **You're a data scientist at a real estate company. Based on this experiment, what would you tell your manager about the importance of learning rate tuning?**\n",
    "\n",
    "   *Your answer:*\n",
    "   \n",
    "\n",
    "###  Key Takeaways\n",
    "\n",
    "Write 3-5 bullet points summarizing the most important lessons from this learning rate sensitivity analysis:\n",
    "\n",
    "- *Your takeaway 1:*\n",
    "- *Your takeaway 2:*  \n",
    "- *Your takeaway 3:*\n",
    "- *Your takeaway 4:*\n",
    "- *Your takeaway 5:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344729aa-f0e6-4910-b08a-c0c667c964fc",
   "metadata": {
    "id": "344729aa-f0e6-4910-b08a-c0c667c964fc"
   },
   "source": [
    "## üèÜ Congratulations! You've Mastered Gradient Descent!\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "In this quiz, you successfully:\n",
    "\n",
    "**üî¨ Built from Scratch:**\n",
    "\n",
    "- Cost function using Mean Squared Error\n",
    "- Gradient calculations with partial derivatives  \n",
    "- Complete gradient descent optimization algorithm\n",
    "\n",
    "**üè† Solved Real Problems:**\n",
    "\n",
    "- Predicted house prices using machine learning\n",
    "- Trained a model on actual data\n",
    "- Made business-ready predictions\n",
    "\n",
    "**üìä Understood the Process:**\n",
    "\n",
    "- Visualized algorithm learning through cost plots\n",
    "- Observed parameter convergence in real-time\n",
    "- Connected math to practical applications\n",
    "\n",
    "\n",
    "### Next Steps in Your Learning Journey\n",
    "\n",
    "You're now ready for:\n",
    "\n",
    "- **Multi-variable regression** and beyond!\n",
    "- **Vectorized implementations** for better performance\n",
    "- **Advanced algorithms** (Adam, RMSprop, etc.)\n",
    "\n",
    "\n",
    "**Key Insight:** Every advanced ML algorithm builds on these fundamentals you've mastered!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfdab3-e6d8-426f-8699-cd01f763d538",
   "metadata": {
    "id": "1dcfdab3-e6d8-426f-8699-cd01f763d538"
   },
   "source": [
    "## Reference\n",
    "\n",
    "https://www.deeplearning.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f328376-71d5-46af-883d-06d9feba7383",
   "metadata": {
    "id": "0f328376-71d5-46af-883d-06d9feba7383"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stat362-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
