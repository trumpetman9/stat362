{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2c15dac0-bea3-4ef7-8968-798d69efefd3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"HW1: L2 Regularized Logistic Regression\"\n",
    "subtitle: \"From Scratch Implementation to Real-World Heart Disease Prediction\"\n",
    "number-sections: true\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5de41f-a061-4451-8b50-41aec0f95c7b",
   "metadata": {},
   "source": [
    "## Instructions {-}\n",
    "\n",
    "1. You may talk to a friend, discuss the questions and potential directions for solving them. However, you need to write your own solutions and code separately, and not as a group activity. \n",
    "\n",
    "2. Write your code in the **Code cells** and your answers in the **Markdown cells** of the Jupyter notebook. Ensure that the solution is written neatly enough to for the graders to understand and follow.\n",
    "\n",
    "3. Use [Quarto](https://quarto.org/docs/output-formats/html-basics.html) to render the **.ipynb** file as HTML. You will need to open the command prompt, navigate to the directory containing the file, and use the command: `quarto render filename.ipynb --to html`. Submit the HTML file.\n",
    "\n",
    "4. The assignment is worth 100 points, and is due on **18th October 2025 at 11:59 pm**. \n",
    "\n",
    "5. **Five points are properly formatting the assignment**. The breakdown is as follows:\n",
    "    - Must be an HTML file rendered using Quarto **(1 point)**. *If you have a Quarto issue, you must mention the issue & quote the error you get when rendering using Quarto in the comments section of Canvas, and submit the ipynb file.* \n",
    "    - No name can be written on the assignment, nor can there be any indicator of the student’s identity—e.g. printouts of the working directory should not be included in the final submission.  **(1 point)**\n",
    "    - There aren’t excessively long outputs of extraneous information (e.g. no printouts of entire data frames without good reason, there aren’t long printouts of which iteration a loop is on, there aren’t long sections of commented-out code, etc.) **(1 point)**\n",
    "    - Final answers to each question are written in the Markdown cells. **(1 point)**\n",
    "    - There is no piece of unnecessary / redundant code, and no unnecessary / redundant text. **(1 point)**\n",
    "\n",
    "6.  The maximum possible score in the assigment is 100  + 15 (bonus task) + 5 (AI usage disclosure)= 120 out of 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37860adb",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "- **Reinforce Knowledge of Gradient Descent:** Apply your understanding of gradient descent to classification problems by implementing logistic regression and softmax regression from scratch.\n",
    "- **Hands-on Implementation:** Build classification models manually to gain deeper insights into their mathematical foundations and working principles.\n",
    "- **Explore Customization Options:** Learn how implementing models from scratch allows you to:\n",
    "  - Adjust and optimize model parameters for specific requirements.\n",
    "  - Add features or constraints that might not be possible with standard libraries.\n",
    "- **Compare with Pre-built Models:** Use scikit-learn’s logistic regression as a baseline to evaluate the performance and efficiency of your custom implementation. This will help you understand when to use custom models and when to leverage pre-built ones.\n",
    "- **Prepare for Real-world Scenarios:** Understand the scenarios where off-the-shelf models are not sufficient, allowing you to confidently tackle complex machine learning problems and create novel solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f39540-d78d-4cec-8ee5-51229dcad5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f9358-3bba-42c7-9b0c-8792a1327918",
   "metadata": {},
   "source": [
    "## Refreshing the Foundations: Sigmoid & Logistic Regression\n",
    "\n",
    "### Why Logistic Regression? The Classification Challenge\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/image-96.webp\" alt=\"Logistic Cost Function\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "**The Problem**: Linear regression gives us $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot  \\mathbf{x}^{(i)} + b$, but for classification:\n",
    "\n",
    "- **Linear regression outputs**: Any real number (-∞ to +∞)\n",
    "- **Classification needs**: Probabilities between 0 and 1\n",
    "- **Binary targets**: $y \\in \\{0, 1\\}$ (e.g., Disease/No Disease, Spam/Not Spam)\n",
    "\n",
    "**The Solution**: Transform linear outputs into probabilities using the **sigmoid function**, creating a model that outputs meaningful probabilities for binary classification decisions.\n",
    "\n",
    "**Real-World Context**: In this assignment, you'll predict heart disease risk where:\n",
    "- **Input**: Patient medical data (age, cholesterol, chest pain type, etc.)\n",
    "- **Output**: Probability of heart disease (0 = healthy, 1 = disease)\n",
    "- **Goal**: Build a model doctors can trust for medical decision-making\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b347e1-8907-490d-bf66-15f8a8c9b568",
   "metadata": {},
   "source": [
    "### The Sigmoid Function: Mathematical Bridge to Probabilities\n",
    "\n",
    "**Mathematical Definition:**\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}} \\tag{1}$$\n",
    "\n",
    "**What This Function Does:**\n",
    "- **Input $z$**: Any real number (-∞ to +∞) from linear model\n",
    "- **Output $g(z)$**: Always between 0 and 1 (perfect for probabilities!)\n",
    "- **S-shaped curve**: Smooth transition from 0 to 1\n",
    "\n",
    "**Key Properties:**\n",
    "- **$g(0) = 0.5$**: Neutral prediction (equal probability of both classes)\n",
    "- **$g(z) \\to 1$** as $z \\to +\\infty$: Strong positive evidence → high probability  \n",
    "- **$g(z) \\to 0$** as $z \\to -\\infty$: Strong negative evidence → low probability\n",
    "- **Always smooth**: No sudden jumps, making it perfect for gradient descent optimization\n",
    "\n",
    "**Practical Interpretation:**\n",
    "In logistic regression, $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ represents the \"log-odds\" or \"logit\":\n",
    "- **Positive $z$**: Model leans toward class 1 (e.g., \"has disease\")\n",
    "- **Negative $z$**: Model leans toward class 0 (e.g., \"healthy\")  \n",
    "- **Magnitude of $|z|$**: Confidence level of the prediction\n",
    "\n",
    "**Implementation Note**: NumPy's [`exp()`](https://numpy.org/doc/stable/reference/generated/numpy.exp.html) function handles vectorized computation efficiently, but requires careful numerical handling to avoid overflow (which you'll implement!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c69ac8-0c00-4827-8921-b6550ed8d5c2",
   "metadata": {},
   "source": [
    "### Logistic Regression: Model\n",
    "<img align=\"left\" src=\"./images/C1_W3_LogisticRegression_right.png\"     style=\" width:300px; padding: 10px; \" > A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:\n",
    "\n",
    "$$ \n",
    "f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\tag{2} \n",
    "$$ \n",
    "\n",
    "  where\n",
    "\n",
    "  $$\n",
    "  g(z) = \\frac{1}{1+e^{-z}}\\tag{3}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5760f-12e3-4864-bf3b-cb42ad2f3ca3",
   "metadata": {},
   "source": [
    "### Logistic Regression: Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cb12b-65aa-4c91-a642-79684d0b58c5",
   "metadata": {},
   "source": [
    "Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. \n",
    "\n",
    ">**Definition Note:**   In this course, these definitions are used:  \n",
    "**Loss** is a measure of the difference of a single example to its target value while the  \n",
    "**Cost** is a measure of the losses over the training set\n",
    "\n",
    "\n",
    "This is defined: \n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n",
    "\n",
    "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTIyEcDUIGx9kLDohkOyjq8X2OkQZbxLoVW3JyEefVtog&s alt=\"Description of image\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59402019-5be0-4b2f-854a-1eb20e79e6d0",
   "metadata": {},
   "source": [
    "Combined, the curves are similar to the quadratic curve of the squared error loss. Note, the x-axis is $f_{\\mathbf{w},b}$ which is the output of a sigmoid. The sigmoid output is strictly between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba0f78-881b-4dfd-a2aa-e1ac1aa6b061",
   "metadata": {},
   "source": [
    "The loss function above can be rewritten to be easier to implement.\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "  \n",
    "This is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces:  \n",
    "when $ y^{(i)} = 0$, the left-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "and when $ y^{(i)} = 1$, the right-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Why This Loss Function Works:**\n",
    "1. **Probabilistic Foundation**: Derived from maximum likelihood estimation\n",
    "2. **Convex Optimization**: Guarantees unique global minimum (perfect for gradient descent)\n",
    "3. **Penalizes Confidence**: Wrong confident predictions get heavily penalized\n",
    "4. **Smooth Gradients**: Enables stable gradient descent convergence\n",
    "\n",
    "**Connection to Your Implementation**: This loss function is exactly what you'll implement in Task 1, with the addition of L2 regularization to prevent overfitting on the heart disease dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7e36d-723d-4cdb-a772-412ff188130e",
   "metadata": {},
   "source": [
    "### Logistic Regression: Cost Function (Training Objective)\n",
    "\n",
    "**From Individual Loss to Overall Cost:**\n",
    "\n",
    "The **cost function** aggregates individual losses across all training examples to create our optimization objective:\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "Where the individual **loss** for each example is:\n",
    "\n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "\n",
    "**The Complete Model Pipeline:**\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Optimization Goal**: Find weights $\\mathbf{w}$ and bias $b$ that minimize $J(\\mathbf{w},b)$ using **gradient descent**.\n",
    "\n",
    "\n",
    "## Bridge to Implementation: What You'll Build\n",
    "\n",
    "### Your Assignment Journey:\n",
    "1. **Task 1**: Implement cost function with L2 regularization → **20 points**  \n",
    "2. **Task 2**: Apply to heart disease dataset with preprocessing → **15 points**  \n",
    "3. **Task 3**: Experiment with regularization strength (λ) → **20 points**  \n",
    "4. **Task 4**: Benchmark against scikit-learn LogisticRegression → **20 points**  \n",
    "5. **Task 5**: Explore the `tol` parameter in gradient descent–based models → **15 points**  \n",
    "6. **Task 6**: Summarize key learnings and insights → **5 points** \n",
    "\n",
    "### ⭐ Bonus Task  \n",
    "Extend your L2-regularized logistic regression by adding a **`tol` stopping criterion**, and compare its behavior with scikit-learn’s implementation -> **15 points**\n",
    "   \n",
    "### Key Implementation Challenges You'll Solve:\n",
    "\n",
    "- **Numerical Stability**: Prevent `log(0)` errors that break gradient descent\n",
    "- **Vectorization**: Implement efficient matrix operations for real-world performance\n",
    "- **Regularization**: Add L2 penalty to prevent overfitting on complex medical data\n",
    "- **Gradient Computation**: Derive and implement the gradient for weight updates\n",
    "- **Convergence Monitoring**: Track training progress and detect overfitting\n",
    "\n",
    "### Real-World Impact:\n",
    "Your implementation will predict heart disease risk using patient data—a model that could potentially assist medical professionals in making life-saving decisions. The mathematical rigor you develop here translates directly to production ML systems used in healthcare, finance, and technology.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70e382-25b8-470c-8aea-c4fbb10856c4",
   "metadata": {},
   "source": [
    "## Task 1: Implementing L2 Regularized Logistic Regression with Vectorized Gradient Descent **(20 points)**\n",
    "\n",
    "###  **Objective**\n",
    "Implement a complete L2 regularized (Ridge) logistic regression algorithm from scratch using vectorized operations to prevent overfitting in binary classification problems.\n",
    "\n",
    "### 📋 **Implementation Requirements**\n",
    "\n",
    "#### **Main Function Signature**\n",
    "Your implementation should create a main training function that takes these inputs:\n",
    "\n",
    "- **X_train** (ndarray): Training feature matrix (m × n) - **Note**: Should include bias column of ones as first column\n",
    "- **y_train** (ndarray): Training labels (m × 1) \n",
    "- **X_test** (ndarray): Test feature matrix for monitoring overfitting - **Note**: Should include bias column  \n",
    "- **y_test** (ndarray): Test labels for performance tracking\n",
    "- **w_in** (ndarray): Initial weights ((n+1) × 1) - **Includes bias weight as first element**\n",
    "- **alpha** (float): Learning rate (typically 0.001 - 0.01)\n",
    "- **num_iters** (int): Number of training iterations\n",
    "- **lambda_reg** (float): L2 regularization strength (λ ≥ 0)\n",
    "\n",
    "#### **Expected Outputs**\n",
    "- **Optimized parameters**: Final weights after training\n",
    "- **Training cost history**: Cost values at each iteration on training data\n",
    "- **Test cost history**: Cost values at each iteration on test data (for overfitting analysis)\n",
    "\n",
    "### **Implementation Structure**\n",
    "\n",
    "#### **Step 1: Cost Function with L2 Regularization**\n",
    "```python\n",
    "def compute_cost_logistic_ridge(X, y, w, lambda_reg):\n",
    "```\n",
    "**Mathematical Formula:**\n",
    "$$J(w) = \\frac{1}{m}\\sum_{i=1}^{m} \\left[ -y^{(i)} \\log(h_w(x^{(i)})) - (1-y^{(i)}) \\log(1-h_w(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} w_j^2$$\n",
    "\n",
    "Where $h_w(x) = \\sigma(w^T x)$ and $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "**Key Implementation Notes:**\n",
    "\n",
    "- Use vectorized operations for efficiency\n",
    "- **Bias handling**: Don't regularize the bias term (w[0]) - exclude from L2 penalty\n",
    "- **Data preparation**: Add column of ones to X matrices for bias term\n",
    "- Handle numerical stability with log functions\n",
    "\n",
    "**Why Numerical Stability Matters:**\n",
    "\n",
    "- **When h = 0**: `log(0) = -∞` → **NaN in cost calculations**\n",
    "- **When h = 1**: `log(1-h) = log(0) = -∞` → **NaN in cost calculations**  \n",
    "- **When it occurs**: Sigmoid outputs extreme values (overconfident predictions)\n",
    "- **Consequence**: Entire gradient descent breaks due to NaN propagation\n",
    "\n",
    "   **Practical Example:**\n",
    "   ```python\n",
    "   # Without clipping - BREAKS!\n",
    "   h = np.array([0.0, 0.5, 1.0])  # Sigmoid predictions\n",
    "   cost = -np.log(h)  # Returns: [inf, 0.693, 0.0]\n",
    "   # ↑ inf values propagate to gradients → NaN → training failure\n",
    "\n",
    "   # With clipping - WORKS!\n",
    "   h_safe = np.clip(h, 1e-15, 1-1e-15)  # Safe range: [1e-15, 1-1e-15]\n",
    "   cost = -np.log(h_safe)  # Returns: [34.54, 0.693, 34.54]  \n",
    "   # ↑ All finite values → stable gradients → successful training\n",
    "   ```\n",
    "\n",
    "**Professional Tip**: All major ML libraries (TensorFlow, PyTorch, sklearn) use similar clipping internally!\n",
    "\n",
    "#### **Step 2: Gradient Computation with L2 Regularization**\n",
    "```python\n",
    "def compute_gradient_logistic_ridge(X, y, w, lambda_reg):\n",
    "```\n",
    "**Mathematical Formula:**\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} X^T (\\sigma(Xw) - y) + \\frac{\\lambda}{m} w_j \\quad \\text{(for } j > 0\\text{)}$$\n",
    "$$\\frac{\\partial J}{\\partial w_0} = \\frac{1}{m} X^T (\\sigma(Xw) - y) \\quad \\text{(bias term, no regularization)}$$\n",
    "\n",
    "**Implementation Note**: The gradient vector should be same shape as w_in ((n+1) × 1), where the first element corresponds to the bias term gradient (no regularization) and remaining elements include the L2 regularization term.\n",
    "\n",
    "#### **Step 3: Gradient Descent Algorithm**\n",
    "```python\n",
    "def gradient_descent_logistic_ridge(X_train, y_train, X_test, y_test, w_in, alpha, num_iters, lambda_reg):\n",
    "```\n",
    "\n",
    "#### 💡 **Implementation Tips**\n",
    "\n",
    "1. **Bias Term Setup**: \n",
    "   - Add column of ones as **first column** of X matrices: `X = np.column_stack([np.ones(m), X_features])`\n",
    "   - Initialize w_in with shape (n+1, 1) where w[0] is bias, w[1:] are feature weights\n",
    "2. **Vectorization**: Use matrix operations instead of loops for efficiency\n",
    "3. **Numerical Stability**: \n",
    "   - Clip extreme values in sigmoid to prevent overflow\n",
    "   - Use `np.clip(predictions, 1e-15, 1-1e-15)` for log calculations\n",
    "4. **Regularization Implementation**: \n",
    "   - Create regularization vector: `reg_term = np.copy(w); reg_term[0] = 0` (exclude bias)\n",
    "   - Add to gradient: `gradient + (lambda_reg/m) * reg_term`\n",
    "5. **Monitoring**: Track both training and test costs to observe overfitting\n",
    "\n",
    "\n",
    "#### ✅ **Validation Checklist**\n",
    "- [ ] Cost decreases monotonically during training\n",
    "- [ ] Implementation uses vectorized operations (no explicit loops over samples)\n",
    "- [ ] Regularization term correctly excludes bias\n",
    "- [ ] Both training and test costs are tracked\n",
    "- [ ] Function handles edge cases (very small/large predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788436e-7f8f-46c4-9d0d-d1f629921869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa3f1d8-5663-4115-aa84-a241a059611e",
   "metadata": {},
   "source": [
    "\n",
    "## Task 2: Apply your implementation on a real dataset **(15 points)**\n",
    "\n",
    "### **Objective**\n",
    "Apply your L2 regularized logistic regression implementation to predict heart disease using a real medical dataset, demonstrating proper data preprocessing, model training, and performance visualization.\n",
    "\n",
    "### **Dataset Overview** \n",
    "The **heart_disease_classification.csv** contains medical data for heart disease prediction with:\n",
    "\n",
    "- **Target**: Binary classification (0=No Disease, 1=Disease)  \n",
    "- **Features**: Mix of numerical (age, cholesterol) and categorical (chest pain type, slope)\n",
    "- **Challenge**: Categorical variables need proper encoding for your algorithm\n",
    "\n",
    "### 📋 **Step-by-Step Implementation Guide**\n",
    "\n",
    "#### **Step 1: Data Loading and Initial Exploration**\n",
    "\n",
    "In this step, you will begin by **importing the dataset into your working environment** (e.g., using `pandas.read_csv()` for CSV files). After successfully loading the data, perform some **basic exploratory checks** to understand its structure and contents. This includes:\n",
    "\n",
    "- Displaying the **first few rows** of the dataset with `.head()` to get a sense of the data format  \n",
    "- Checking the **dimensions** of the dataset with `.shape` to see how many rows and columns are available  \n",
    "- Inspecting the **column names and data types** with `.info()` to identify numerical, categorical, and datetime features  \n",
    "- Reviewing **summary statistics** with `.describe()` to quickly understand ranges, averages, and distributions of numeric columns  \n",
    "\n",
    "This step ensures that you have properly loaded the dataset and gained a **high-level understanding of its characteristics**, which is essential before moving into deeper transformation, or modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af2159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b49957",
   "metadata": {},
   "source": [
    "#### **Step 2: Data Shuffling and Feature-Target Separation (*random_state=0*)**\n",
    "\n",
    "After loading and exploring the dataset, the next step is to **prepare it for modeling**. This involves two tasks:\n",
    "\n",
    "1. **Shuffling the data**\n",
    "   - Many real-world datasets, especially in domains like **medicine**, are collected in sequential order (e.g., sorted by collection date, patient ID, or clinical visit number).  \n",
    "   - If you split the dataset without shuffling, the training set may contain only earlier cases and the test set may contain only later ones. This can introduce **temporal or ID-based bias** and reduce the model’s ability to generalize.  \n",
    "   - Shuffling ensures that the **distribution of samples is random**, giving both training and test sets a representative mix of patients and outcomes.  \n",
    "   - We use `random_state=0` to make the shuffle **reproducible**, so everyone gets the same randomized order.  \n",
    "\n",
    "2. **Separating features and target**\n",
    "   - Define the **target variable** (the outcome you want to predict, e.g., disease status, biomarker level, or treatment response).  \n",
    "   - Select the **features** (independent variables, such as patient demographics, lab values, or imaging-derived metrics).  \n",
    "   - Split the dataset into:  \n",
    "     - `X` → the **feature matrix** (all input variables except the target)  \n",
    "     - `y` → the **target vector** (the label or response variable)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a557e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3de32af",
   "metadata": {},
   "source": [
    "#### **Step 3: Train–Test Split (random_state=42) and Feature Scaling**\n",
    "\n",
    "Once the dataset is shuffled and the features (`X`) and target (`y`) are separated, the next step is to **divide the data into training and testing subsets**.  \n",
    "\n",
    "- **Training set**: Used to fit the model (learn the patterns from the data).  \n",
    "- **Test set**: Held back to evaluate how well the model generalizes to unseen data.  \n",
    "\n",
    "We typically use **80% of the data for training** and **20% for testing**, though this ratio can be adjusted based on dataset size and problem context.  \n",
    "\n",
    "To ensure reproducibility, we specify `random_state=42` when performing the split.  \n",
    "\n",
    "##### ✅ Why split the data?\n",
    "\n",
    "- Prevents **overfitting** by ensuring the model is evaluated on unseen data.  \n",
    "- Mimics a real-world scenario where the model encounters new patient cases.  \n",
    "- Provides an unbiased estimate of model performance. \n",
    "\n",
    "##### ✅ Why scaling matters\n",
    "- Features may be measured on very different scales (e.g., **age** ranges 20–80, while **cholesterol** may range 100–600).  \n",
    "- Algorithms that use **gradient descent** (e.g., logistic regression, neural networks) can converge slowly or get stuck if features are not scaled properly.  \n",
    "- Scaling puts all features on a **comparable range**, improving both **training stability** and **model performance**. \n",
    "\n",
    "**Important Notes**:  \n",
    "\n",
    "- Use `fit_transform()` **only on the training set** so the scaler does not \"see\" information from the test set (**avoiding data leakage**).  \n",
    "- Apply the fitted scaler to the **test set** using `transform()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060a8583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ea75899",
   "metadata": {},
   "source": [
    "#### **Step 4: Categorical Variable Encoding**\n",
    "\n",
    "The dataset contains **categorical variables** that must be converted into numerical form before modeling. Most machine learning algorithms cannot directly handle categorical text values, so we use **binary (dummy) encoding**.\n",
    "\n",
    "\n",
    "**Required Encoding for These Columns:**\n",
    "- **`cp`** (Chest Pain Type): Convert into binary dummy variables  \n",
    "- **`thal`** (Thalassemia): Convert into binary dummy variables  \n",
    "- **`slope`** (ST Slope): Convert into binary dummy variables  \n",
    "\n",
    "\n",
    "##### ✅ Why Encode?\n",
    "- Machine learning algorithms work with **numbers**, not text labels.  \n",
    "- Encoding categorical variables as dummy variables prevents the model from assuming an **ordinal relationship** between categories (e.g., category \"3\" is not “greater than” category \"1\").  \n",
    "- This ensures that the algorithm treats categories as **independent groups**.  \n",
    "\n",
    "\n",
    "\n",
    "##### Example (using `pandas.get_dummies`)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Encode categorical variables into dummy variables\n",
    "df_encoded = pd.get_dummies(df, columns=['cp', 'thal', 'slope'], drop_first=True)\n",
    "\n",
    "# View the new columns\n",
    "df_encoded.head()\n",
    "```\n",
    "Here:\n",
    "\n",
    "- `pd.get_dummies()` automatically creates binary (0/1) columns for each category.\n",
    "\n",
    "- `drop_first=True` avoids multicollinearity by dropping one category from each variable as the reference group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8ad38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b93c25c1",
   "metadata": {},
   "source": [
    "#### **Step 5: Model Training and Hyperparameter Selection**\n",
    "\n",
    "With the training and test sets prepared, the next step is to **train the model**. This involves selecting appropriate **hyperparameters** that control how the algorithm learns.  \n",
    "\n",
    "\n",
    "##### ✅ Key Considerations\n",
    "- **Learning Rate (`α`)**: Controls the step size during gradient descent updates.  \n",
    "  - Too large → the algorithm may diverge.  \n",
    "  - Too small → convergence becomes very slow.  \n",
    "\n",
    "- **Number of Iterations / Epochs**: Determines how many times the algorithm goes through the training data.  \n",
    "  - Choose enough iterations for convergence, but avoid unnecessary computation.  \n",
    "\n",
    "- **Regularization (if applicable)**: Prevents overfitting by penalizing large weights.  \n",
    "\n",
    "#####  Practical Setup\n",
    "- Initialize parameters (`w_in`).  \n",
    "- Select a reasonable learning rate (`alpha`).  \n",
    "- Choose the number of iterations (`num_iters`).  \n",
    "- Set a regularization strength (`lambda_reg`).  \n",
    "\n",
    "As long as the model **converges stably**, your hyperparameters are acceptable.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506127c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ec5e03c",
   "metadata": {},
   "source": [
    "#### **Step 6: Plot the Learning Curve of Gradient Descent**\n",
    "\n",
    "After training the model with gradient descent, it is important to **visualize the learning process**. A learning curve shows how the model’s **loss or accuracy changes** over iterations (epochs) for both the **training set** and the **test set**.\n",
    "\n",
    "\n",
    "##### ✅ Why Learning Curves Matter\n",
    "- **Convergence Check**: Helps verify whether gradient descent is converging to a minimum.  \n",
    "- **Underfitting vs. Overfitting**:  \n",
    "  - If both training and test errors are high → underfitting.  \n",
    "  - If training error is low but test error is high → overfitting.  \n",
    "- **Hyperparameter Tuning**: Reveals whether changes in learning rate (`α`), number of iterations, or regularization improve training stability.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c82e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26bf8091-bab2-4738-aad7-589594f261a9",
   "metadata": {},
   "source": [
    "## Task 3: Regularization Analysis - Finding the Optimal λ Value **(20 points)**\n",
    "\n",
    "`lambda_reg` controls the strength of the regularization applied to the model.  \n",
    "- When `lambda_reg` is set to **zero**, regularization is effectively turned off.  \n",
    "- As `lambda_reg` increases, the penalty for large weights becomes more significant, helping to **reduce overfitting**.  \n",
    "\n",
    "\n",
    "\n",
    "### 🔧 Task Instructions\n",
    "- Experiment with different values of `lambda_reg` in the set **[0.0, 0.01, 0.03, 0.1, 0.3]**.  \n",
    "- Plot the **learning curves** for both the training and test sets on the same figure to visualize the impact of each value.  \n",
    "- Determine which value of `lambda_reg` yields the **best performance** on this dataset.  \n",
    "- Report the performance (accuracy, precision, recall) using the best value of:  \n",
    "  - **Accuracy**  \n",
    "  - **Precision**  \n",
    "  - **Recall**  \n",
    "\n",
    "\n",
    "\n",
    "**Hyperparameters for this task**:  \n",
    "- `learning_rate = 0.005`  \n",
    "- `num_iterations = 1200`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1207a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e5f362b-2348-4a83-9b6c-545a14ab7a3a",
   "metadata": {},
   "source": [
    "## Task 4: Custom vs. scikit-learn — Comprehensive Benchmark Analysis **(20 points)**\n",
    "\n",
    "- Use the **`LogisticRegression`** model from **scikit-learn** to re-evaluate the dataset while keeping the same (or as similar as possible) hyperparameter settings for a **fair comparison**.  \n",
    "- Report the performance using the **same evaluation metrics** as previously used.  \n",
    "- Compare the results to your **custom implementation**. Analyze whether scikit-learn’s built-in logistic regression achieves similar, better, or worse performance.  \n",
    "- Explain the potential reasons for any differences you observe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f11d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7f2c627-aaa2-4dc0-a68c-dd4dea175ae5",
   "metadata": {},
   "source": [
    "## Task 5: Understanding Convergence - The `tol` Parameter Deep Dive **(15 points)**\n",
    "\n",
    "In many models that use gradient descent–based optimization, `tol` is a key hyperparameter.  \n",
    "In this task, you will explore its role using **logistic regression**.\n",
    "\n",
    "### **Understanding the Tolerance Parameter**\n",
    "The `tol` parameter in sklearn's LogisticRegression controls **when to stop the optimization process**:\n",
    "\n",
    "- **What it measures**: Maximum change in the optimization objective between iterations\n",
    "- **Stopping criterion**: Training stops when improvement falls below this threshold\n",
    "- **Trade-off**: Tighter tolerance (smaller values) → more iterations → potentially better convergence\n",
    "- **Efficiency**: Looser tolerance (larger values) → fewer iterations → faster training but potentially suboptimal results\n",
    "\n",
    "### **Mathematical Context**\n",
    "For solver convergence, sklearn typically uses:\n",
    "$$|\\nabla J(\\mathbf{w}^{(t)}) - \\nabla J(\\mathbf{w}^{(t-1)})| < \\text{tol}$$\n",
    "Or for objective function change:\n",
    "$$|J(\\mathbf{w}^{(t)}) - J(\\mathbf{w}^{(t-1)})| < \\text{tol}$$\n",
    "\n",
    "\n",
    "### **Expected Insights**\n",
    "\n",
    "Experiment with the following `tol` values to explore their impact on model convergence, training speed, and performance:\n",
    "\n",
    "**Typical Patterns You Should Observe:**\n",
    "- **Tighter tolerance (`1e-6`, `1e-7`)** → Training is slower but may yield slightly better performance.  \n",
    "- **Moderate tolerance (`1e-4`, `1e-5`)** → A good balance between speed and accuracy (close to scikit-learn’s default).  \n",
    "- **Loose tolerance (`1e-1`, `1e-2`)** → Faster training but may stop too early, leading to suboptimal results.  \n",
    "- **Performance plateau** → Beyond a certain level of tightness, further decreasing tolerance does not yield meaningful gains.  \n",
    "- **Convergence issues** → Extremely tight tolerance may require more iterations than allowed, preventing proper convergence.  \n",
    "\n",
    "\n",
    "### ✅ **Success Criteria**\n",
    "- [ ] Systematic testing across tolerance range (8+ values)\n",
    "- [ ] Comprehensive performance and efficiency metrics\n",
    "- [ ] Professional visualization with multiple perspectives\n",
    "- [ ] Clear identification of trade-offs and recommendations\n",
    "- [ ] Evidence-based analysis of sklearn's default choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93239c-180f-49fa-b77d-b1ece150fdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec7a8991",
   "metadata": {},
   "source": [
    "## Bonus Task: Implementing `tol` in Your L2 Gradient Descent (15 points)\n",
    "\n",
    "*This is a bonus task, feel free to skip this task*\n",
    "\n",
    "To make your **custom L2-regularized gradient descent** implementation comparable with scikit-learn’s `LogisticRegression`, you should add the `tol` parameter as a **stopping criterion**.  \n",
    "\n",
    "### ✅ Instructions:\n",
    "1. During each iteration of gradient descent, compute the **change in cost (loss)** between the current and previous step.  \n",
    "2. If the change is **smaller than `tol`**, stop the training loop early.  \n",
    "3. Use the same tolerance values (`tol`) as in scikit-learn for a fair comparison.  \n",
    "4. Report:  \n",
    "   - The number of iterations completed before convergence.  \n",
    "   - Final training and test performance metrics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf1dfd",
   "metadata": {},
   "source": [
    "## Task 6: Summarize your findings below **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19a016",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "180cfbf3",
   "metadata": {},
   "source": [
    "## AI Use Disclosure **(5 pts)**\n",
    "\n",
    "In **1–3 short paragraphs**, clearly state whether **generative AI tools** were used to complete any part of this assignment.\n",
    "\n",
    "- If **no AI was used**, write:  \n",
    "  *“I did not use generative AI tools on this assignment.”*  \n",
    "\n",
    "- If **AI was used**, you must specify:  \n",
    "  1. **Tool(s)** used (e.g., ChatGPT, GitHub Copilot, Claude, etc.)  \n",
    "  2. **How** they were used (e.g., idea generation, debugging, code suggestions, writing help)  \n",
    "  3. **Where** they influenced your work (which questions/sections)  \n",
    "  4. **Edits & verification** you made (how you checked correctness, what you modified)  \n",
    "\n",
    "### AI Use Template\n",
    "- **Used AI?** Yes / No  \n",
    "- **Tool(s):**  \n",
    "- **Purpose / How used (1–3 sentences):**  \n",
    "- **Scope (which questions/sections):**  \n",
    "- **Edits & verification (what you changed and how you checked correctness):**  \n",
    "\n",
    "> **Example (for illustration only):**  \n",
    "> - Used AI? Yes  \n",
    "> - Tools: ChatGPT (free), GitHub Copilot  \n",
    "> - How used: Asked for a pandas snippet to impute missing values and for a seaborn example.  \n",
    "> - Scope: Q1(b) imputation, Q2(a) first draft of bar chart code.  \n",
    "> - Edits & verification: Rewrote code to use `groupby().transform('median')`; validated results by comparing summary stats; added axis labels manually.  \n",
    "\n",
    "**Grading (5 points):**  \n",
    "- **5 points** = complete, specific, and honest (tools named, usage described clearly)  \n",
    "- **3–4 points** = vague or missing some details  \n",
    "- **1–2 points** = minimal effort or very unclear  \n",
    "- **0 points** = missing or misleading disclosure  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b68f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47f6e28f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
